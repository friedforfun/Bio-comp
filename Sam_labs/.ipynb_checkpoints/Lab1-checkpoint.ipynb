{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1\n",
    "\n",
    "## Basic idea for gradient descent\n",
    "### Not logistic regression classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron(input, weights):\n",
    "    out = 0\n",
    "    for i in range(len(input)):\n",
    "        out += (input[i] * weights[i])\n",
    "    return out\n",
    "\n",
    "# \n",
    "def ele_mul(scalar, vector):\n",
    "    out = [0,0,0]\n",
    "    for i in range(len(out)):\n",
    "        out[i] = vector[i] * scalar\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Pred: 0.8600000000000001\n",
      "Error: 0.01959999999999997\n",
      "Delta: -0.1399999999999999\n",
      "Weights: [0.1, 0.2, -0.1]\n",
      "Weight_Deltas: [-1.189999999999999, -0.09099999999999994, -0.16799999999999987]\n",
      "\n",
      "Iteration: 2\n",
      "Pred: 0.9637574999999999\n",
      "Error: 0.0013135188062500048\n",
      "Delta: -0.036242500000000066\n",
      "Weights: [0.1119, 0.20091, -0.09832]\n",
      "Weight_Deltas: [-0.30806125000000056, -0.023557625000000044, -0.04349100000000008]\n",
      "\n",
      "Iteration: 3\n",
      "Pred: 0.9906177228125002\n",
      "Error: 8.802712522307997e-05\n",
      "Delta: -0.009382277187499843\n",
      "Weights: [0.11498061250000001, 0.20114557625, -0.09788509000000001]\n",
      "Weight_Deltas: [-0.07974935609374867, -0.006098480171874899, -0.011258732624999811]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature1 = [8.5, 9.5, 9.9, 9.0]\n",
    "feature2 = [0.65, 0.8, 0.8, 0.9]\n",
    "feature3 = [1.2, 1.3, 0.5, 1.0]\n",
    "\n",
    "win_or_lose_binary = [1, 1, 0, 1]\n",
    "\n",
    "# y\n",
    "true = win_or_lose_binary[0]\n",
    "\n",
    "# Alpha is the learning rate\n",
    "alpha = 0.01\n",
    "weights = [0.1, 0.2, -.1]\n",
    "input = [feature1[0], feature2[0], feature3[0]]\n",
    "\n",
    "for iter in range(3):\n",
    "    # yhat\n",
    "    pred = neuron(input, weights)\n",
    "    \n",
    "    # Mean squared loss function\n",
    "    error = (pred - true) ** 2\n",
    "    \n",
    "    # loss calculation\n",
    "    delta = pred - true\n",
    "    \n",
    "    \n",
    "    weight_deltas = ele_mul(delta, input)\n",
    "    \n",
    "    print(\"Iteration: \" + str(iter+1))\n",
    "    print(\"Pred: \"+ str(pred))\n",
    "    print(\"Error: \" + str(error))\n",
    "    print(\"Delta: \" + str(delta))\n",
    "    print(\"Weights: \" + str(weights))\n",
    "    print(\"Weight_Deltas: \" + str(weight_deltas))\n",
    "    print()\n",
    "    \n",
    "    for i in range(len(weights)):\n",
    "        # change the weights by the alpha * how much each weight needs to change\n",
    "        weights[i] -= alpha*weight_deltas[i]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>10</td>\n",
       "      <td>101</td>\n",
       "      <td>76</td>\n",
       "      <td>48</td>\n",
       "      <td>180</td>\n",
       "      <td>32.9</td>\n",
       "      <td>0.171</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>70</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.340</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>5</td>\n",
       "      <td>121</td>\n",
       "      <td>72</td>\n",
       "      <td>23</td>\n",
       "      <td>112</td>\n",
       "      <td>26.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1</td>\n",
       "      <td>126</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.349</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>70</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>30.4</td>\n",
       "      <td>0.315</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0              6      148             72             35        0  33.6   \n",
       "1              1       85             66             29        0  26.6   \n",
       "2              8      183             64              0        0  23.3   \n",
       "3              1       89             66             23       94  28.1   \n",
       "4              0      137             40             35      168  43.1   \n",
       "..           ...      ...            ...            ...      ...   ...   \n",
       "763           10      101             76             48      180  32.9   \n",
       "764            2      122             70             27        0  36.8   \n",
       "765            5      121             72             23      112  26.2   \n",
       "766            1      126             60              0        0  30.1   \n",
       "767            1       93             70             31        0  30.4   \n",
       "\n",
       "     DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                       0.627   50        1  \n",
       "1                       0.351   31        0  \n",
       "2                       0.672   32        1  \n",
       "3                       0.167   21        0  \n",
       "4                       2.288   33        1  \n",
       "..                        ...  ...      ...  \n",
       "763                     0.171   63        0  \n",
       "764                     0.340   27        0  \n",
       "765                     0.245   30        0  \n",
       "766                     0.349   47        1  \n",
       "767                     0.315   23        0  \n",
       "\n",
       "[768 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('diabetes.csv', sep=',', header='infer')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Outcome</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.298000</td>\n",
       "      <td>109.980000</td>\n",
       "      <td>68.184000</td>\n",
       "      <td>19.664000</td>\n",
       "      <td>68.792000</td>\n",
       "      <td>30.304200</td>\n",
       "      <td>0.429734</td>\n",
       "      <td>31.190000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.865672</td>\n",
       "      <td>141.257463</td>\n",
       "      <td>70.824627</td>\n",
       "      <td>22.164179</td>\n",
       "      <td>100.335821</td>\n",
       "      <td>35.142537</td>\n",
       "      <td>0.550500</td>\n",
       "      <td>37.067164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "Outcome                                                                      \n",
       "0           3.298000  109.980000      68.184000      19.664000   68.792000   \n",
       "1           4.865672  141.257463      70.824627      22.164179  100.335821   \n",
       "\n",
       "               BMI  DiabetesPedigreeFunction        Age  \n",
       "Outcome                                                  \n",
       "0        30.304200                  0.429734  31.190000  \n",
       "1        35.142537                  0.550500  37.067164  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('Outcome').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pregnancies                   int64\n",
       "Glucose                       int64\n",
       "BloodPressure                 int64\n",
       "SkinThickness                 int64\n",
       "Insulin                       int64\n",
       "BMI                         float64\n",
       "DiabetesPedigreeFunction    float64\n",
       "Age                           int64\n",
       "Outcome                       int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 768)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = df.to_numpy()\n",
    "\n",
    "data = np.transpose(raw_data)\n",
    "\n",
    "data[8] # training labels (Y)\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw:  [  6.    148.     72.     35.      0.     33.6     0.627  50.      1.   ]\n",
      "Training data size:  (8, 500)\n",
      "Testing data size:  (8, 268)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.804065  ,  2.26906711,  0.62429218, -0.17645351, -0.93391565,\n",
       "       -0.206752  , -0.92034626,  0.14817312])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: slice upper bound is exclusive\n",
    "raw_training_data = data[:, 0:500]\n",
    "raw_testing_data = data[:, 500:]\n",
    "\n",
    "print(\"Raw: \", raw_training_data[:, 0])\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "training_data = preprocessing.scale(raw_training_data[0:8, :])\n",
    "testing_data = preprocessing.scale(raw_testing_data[0:8, :])\n",
    "\n",
    "print(\"Training data size: \", training_data.shape)\n",
    "print(\"Testing data size: \", testing_data.shape)\n",
    "\n",
    "training_data[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration_vec = []\n",
    "loss_vec = []\n",
    "def store_loss(loss, iteration):\n",
    "    iteration_vec.append(iteration)\n",
    "    loss_vec.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid activation function\n",
    "def sigmoid(input_vector, weight_vector, bias):\n",
    "    z = np.dot(input_vector, weight_vector) + bias\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "\n",
    "# loss function\n",
    "def wierd_broken_loss(y, yhat):\n",
    "    return (-y * np.log(yhat) - (1 - y) * np.log(1 - yhat))\n",
    "\n",
    "def loss(y, yhat):\n",
    "    if yhat == 1.0:\n",
    "        return 1\n",
    "    if yhat == 0:\n",
    "        return 0.0\n",
    "    return -(y* np.log(yhat) + (1-y) * np.log(1-yhat))\n",
    "\n",
    "# From lab sheet tutorial\n",
    "def loss_other(y, h):\n",
    "    return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "\n",
    "# Derivative of sigmoid\n",
    "def dz_calc(pred, actual):\n",
    "    return pred - actual\n",
    "    \n",
    "def update_dweights(features, dz, derivative_weights):\n",
    "    for i in range(len(derivative_weights)):\n",
    "        derivative_weights[i] += features[i] * dz\n",
    "    return derivative_weights\n",
    "\n",
    "def update_bias(db, dz):\n",
    "    db += dz\n",
    "    return db\n",
    "\n",
    "# Init random weights and bias\n",
    "weights = np.random.rand(training_data.shape[0])\n",
    "initial_weights = weights\n",
    "bias = np.random.rand(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent algorithm from slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight derivatives:  [-0.24456103  0.44308693  0.23988593 -0.11817557  0.04296507 -0.05607549\n",
      " -0.26200729 -0.04511854]\n",
      "Weights on iteration  0 :  [0.54891221 0.41138261 0.45202281 0.52363511 0.49140699 0.5112151\n",
      " 0.55240146 0.50902371]\n",
      "Loss:  0.7920769841801023\n",
      "Weight derivatives:  [-0.17111798  0.29274758  0.18130095 -0.07696318  0.0243483  -0.03863467\n",
      " -0.1819598  -0.02972121]\n",
      "Weights on iteration  1 :  [0.5831358  0.3528331  0.41576262 0.53902775 0.48653733 0.51894203\n",
      " 0.58879342 0.51496795]\n",
      "Loss:  0.712775021755585\n",
      "Weight derivatives:  [-0.11981455  0.18779242  0.14004676 -0.04822788  0.01179935 -0.02652471\n",
      " -0.1260344  -0.019037  ]\n",
      "Weights on iteration  2 :  [0.60709871 0.31527461 0.38775327 0.54867333 0.48417746 0.52424697\n",
      " 0.6140003  0.51877535]\n",
      "Loss:  0.6757761390166855\n",
      "Weight derivatives:  [-0.08561652  0.11796218  0.11228199 -0.02912837  0.00373148 -0.01851163\n",
      " -0.08874906 -0.01197008]\n",
      "Weights on iteration  3 :  [0.62422202 0.29168218 0.36529687 0.554499   0.48343116 0.5279493\n",
      " 0.63175011 0.52116937]\n",
      "Loss:  0.6585320591851472\n",
      "Weight derivatives:  [-0.06292988  0.07176222  0.0935957  -0.01651281 -0.00131921 -0.01325363\n",
      " -0.06400869 -0.0073337 ]\n",
      "Weights on iteration  4 :  [0.63680799 0.27732973 0.34657773 0.55780156 0.483695   0.53060002\n",
      " 0.64455185 0.52263611]\n",
      "Loss:  0.6501572826183943\n",
      "Weight derivatives:  [-0.04774238  0.0409482   0.08081537 -0.00812167 -0.00438886 -0.00979036\n",
      " -0.04744121 -0.00427909]\n",
      "Weights on iteration  5 :  [0.64635647 0.26914009 0.33041466 0.5594259  0.48457277 0.5325581\n",
      " 0.65404009 0.52349192]\n",
      "Loss:  0.645793031837325\n",
      "Weight derivatives:  [-0.03745536  0.0201875   0.07189157 -0.00249176 -0.00616228 -0.00749891\n",
      " -0.03621496 -0.00225579]\n",
      "Weights on iteration  6 :  [0.65384754 0.26510259 0.31603635 0.55992425 0.48580523 0.53405788\n",
      " 0.66128308 0.52394308]\n",
      "Loss:  0.6432830054367461\n",
      "Weight derivatives:  [-0.03041158  0.00608407  0.0655223   0.00131007 -0.00708757 -0.00598111\n",
      " -0.02852417 -0.00091199]\n",
      "Weights on iteration  7 :  [0.65992986 0.26388578 0.30293188 0.55966223 0.48722275 0.5352541\n",
      " 0.66698792 0.52412548]\n",
      "Loss:  0.641653650316524\n",
      "Weight derivatives:  [-2.55429896e-02 -3.54850200e-03  6.08716023e-02  3.88532744e-03\n",
      " -7.45995160e-03 -4.97987486e-03 -2.32050751e-02 -2.05365392e-05]\n",
      "Weights on iteration  8 :  [0.66503846 0.26459548 0.29075756 0.55888517 0.48871474 0.53625007\n",
      " 0.67162893 0.52412959]\n",
      "Loss:  0.6404559789661848\n",
      "Weight derivatives:  [-0.02214968 -0.01014221  0.0573933   0.00562833 -0.00747578 -0.00432647\n",
      " -0.01949505  0.00056756]\n",
      "Weights on iteration  9 :  [0.66946839 0.26662392 0.2792789  0.5577595  0.49020989 0.53711537\n",
      " 0.67552794 0.52401608]\n",
      "Loss:  0.6394788144822908\n",
      "Weight derivatives:  [-0.01976563 -0.01465061  0.05472449  0.00680177 -0.00726666 -0.00390851\n",
      " -0.01688628  0.00095143]\n",
      "Weights on iteration  10 :  [0.67342152 0.26955404 0.26833401 0.55639915 0.49166322 0.53789707\n",
      " 0.6789052  0.52382579]\n",
      "Loss:  0.6386210764285558\n",
      "Weight derivatives:  [-0.01807639 -0.01771753  0.05262051  0.007583   -0.00692108 -0.00365022\n",
      " -0.0150361   0.00119782]\n",
      "Weights on iteration  11 :  [0.6770368  0.27309755 0.25780991 0.55488255 0.49304744 0.53862711\n",
      " 0.68191242 0.52358623]\n",
      "Loss:  0.6378335573768562\n",
      "Weight derivatives:  [-0.01686764 -0.01978245  0.05091453  0.00809302 -0.00649851 -0.00350005\n",
      " -0.01371092  0.00135202]\n",
      "Weights on iteration  12 :  [0.68041032 0.27705404 0.247627   0.55326394 0.49434714 0.53932713\n",
      " 0.6846546  0.52331582]\n",
      "Loss:  0.6370918402037306\n",
      "Weight derivatives:  [-0.01599214 -0.0211479   0.0494917   0.00841505 -0.00603868 -0.00342273\n",
      " -0.01275024  0.00144493]\n",
      "Weights on iteration  13 :  [0.68360875 0.28128362 0.23772866 0.55158093 0.49555488 0.54001167\n",
      " 0.68720465 0.52302684]\n",
      "Loss:  0.6363834962945283\n",
      "Weight derivatives:  [-0.01534823 -0.02202346  0.04827243  0.00860663 -0.00556784 -0.00339403\n",
      " -0.01204317  0.00149767]\n",
      "Weights on iteration  14 :  [0.6866784  0.28568831 0.22807417 0.54985961 0.49666844 0.54069048\n",
      " 0.68961328 0.5227273 ]\n",
      "Loss:  0.6357019578618943\n",
      "Weight derivatives:  [-0.01486541 -0.02255503  0.04720117  0.0087077  -0.00510299 -0.00339728\n",
      " -0.01151283  0.00152467]\n",
      "Weights on iteration  15 :  [0.68965148 0.29019932 0.21863394 0.54811807 0.49768904 0.54136993\n",
      " 0.69191585 0.52242237]\n",
      "Loss:  0.6350435557066778\n",
      "Weight derivatives:  [-0.01449471 -0.02284456  0.04623901  0.00874611 -0.00465482 -0.00342105\n",
      " -0.01110575  0.00153577]\n",
      "Weights on iteration  16 :  [0.69255042 0.29476823 0.20938614 0.54636884 0.49862001 0.54205414\n",
      " 0.694137   0.52211522]\n",
      "Loss:  0.6344060753158138\n",
      "Weight derivatives:  [-0.01420203 -0.02296352  0.04535851  0.00874126 -0.00422968 -0.00345749\n",
      " -0.01078469  0.00153763]\n",
      "Weights on iteration  17 :  [0.69539083 0.29936093 0.20031443 0.54462059 0.49946594 0.54274564\n",
      " 0.69629394 0.52180769]\n",
      "Loss:  0.6337880475701342\n",
      "Weight derivatives:  [-0.01396362 -0.02296214  0.04454026  0.00870672 -0.00383102 -0.00350125\n",
      " -0.01052368  0.00153473]\n",
      "Weights on iteration  18 :  [0.69818355 0.30395336 0.19140638 0.54287925 0.50023215 0.54344589\n",
      " 0.69839867 0.52150074]\n",
      "Loss:  0.6331883973293727\n",
      "Weight derivatives:  [-0.01376296 -0.02287583  0.04377043  0.00865197 -0.00346029 -0.00354872\n",
      " -0.01030462  0.00153002]\n",
      "Weights on iteration  19 :  [0.70093614 0.30852853 0.1826523  0.54114885 0.50092421 0.54415564\n",
      " 0.7004596  0.52119474]\n",
      "Loss:  0.6326062674046529\n",
      "Weight derivatives:  [-0.01358854 -0.02272957  0.04303913  0.00858364 -0.00311769 -0.00359749\n",
      " -0.01011491  0.00152542]\n",
      "Weights on iteration  20 :  [0.70365385 0.31307444 0.17404447 0.53943213 0.50154774 0.54487514\n",
      " 0.70248258 0.52088966]\n",
      "Loss:  0.6320409289953332\n",
      "Weight derivatives:  [-0.01343236 -0.02254109  0.04233922  0.00850637 -0.00280256 -0.00364596\n",
      " -0.00994574  0.00152213]\n",
      "Weights on iteration  21 :  [0.70634032 0.31758266 0.16557663 0.53773085 0.50210826 0.54560433\n",
      " 0.70447173 0.52058523]\n",
      "Loss:  0.631491735049789\n",
      "Weight derivatives:  [-0.01328889 -0.02232297  0.04166549  0.00842339 -0.00251374 -0.00369312\n",
      " -0.00979103  0.00152087]\n",
      "Weights on iteration  22 :  [0.7089981  0.32204725 0.15724353 0.53604617 0.502611   0.54634295\n",
      " 0.70642993 0.52028105]\n",
      "Loss:  0.6309580951364765\n",
      "Weight derivatives:  [-0.0131543  -0.02208417  0.04101414  0.00833694 -0.00224975 -0.00373835\n",
      " -0.00964653  0.00152202]\n",
      "Weights on iteration  23 :  [0.71162896 0.32646409 0.1490407  0.53437879 0.50306095 0.54709062\n",
      " 0.70835924 0.51997665]\n",
      "Loss:  0.6304394612438681\n",
      "Weight derivatives:  [-0.01302596 -0.02183111  0.04038232  0.00824862 -0.00200898 -0.0037813\n",
      " -0.00950931  0.00152573]\n",
      "Weights on iteration  24 :  [0.71423415 0.33083031 0.14096423 0.53272906 0.50346275 0.54784688\n",
      " 0.7102611  0.5196715 ]\n",
      "Loss:  0.6299353192491451\n",
      "Weight derivatives:  [-0.01290205 -0.02156842  0.03976789  0.00815949 -0.00178977 -0.0038218\n",
      " -0.00937736  0.00153203]\n",
      "Weights on iteration  25 :  [0.71681456 0.33514399 0.13301066 0.53109717 0.5038207  0.54861124\n",
      " 0.71213658 0.5193651 ]\n",
      "Loss:  0.6294451834185026\n",
      "Weight derivatives:  [-0.01278137 -0.02129942  0.0391692   0.00807031 -0.00159046 -0.00385978\n",
      " -0.0092493   0.00154083]\n",
      "Weights on iteration  26 :  [0.71937084 0.33940387 0.12517682 0.5294831  0.5041388  0.5493832\n",
      " 0.71398644 0.51905693]\n",
      "Loss:  0.628968592599903\n",
      "Weight derivatives:  [-0.0126631  -0.02102654  0.03858495  0.00798159 -0.00140943 -0.00389529\n",
      " -0.00912419  0.00155201]\n",
      "Weights on iteration  27 :  [0.72190346 0.34360918 0.11745983 0.52788679 0.50442068 0.55016226\n",
      " 0.71581127 0.51874653]\n",
      "Loss:  0.6285051074147703\n",
      "Weight derivatives:  [-0.01254668 -0.02075158  0.03801414  0.00789368 -0.00124517 -0.00392838\n",
      " -0.00900139  0.00156539]\n",
      "Weights on iteration  28 :  [0.7244128  0.3477595  0.109857   0.52630805 0.50466972 0.55094793\n",
      " 0.71761155 0.51843345]\n",
      "Loss:  0.6280543080788692\n",
      "Weight derivatives:  [-0.01243178 -0.02047583  0.03745591  0.00780682 -0.00109624 -0.00395918\n",
      " -0.00888049  0.00158079]\n",
      "Weights on iteration  29 :  [0.72689915 0.35185466 0.10236582 0.52474669 0.50488897 0.55173977\n",
      " 0.71938765 0.5181173 ]\n",
      "Loss:  0.6276157926468641\n",
      "Weight derivatives:  [-0.01231817 -0.02020028  0.03690958  0.00772114 -0.00096129 -0.00398779\n",
      " -0.00876122  0.00159803]\n",
      "Weights on iteration  30 :  [0.72936278 0.35589472 0.0949839  0.52320246 0.50508122 0.55253733\n",
      " 0.7211399  0.51779769]\n",
      "Loss:  0.6271891755600221\n",
      "Weight derivatives:  [-0.01220571 -0.01992568  0.03637456  0.00763675 -0.00083908 -0.00401436\n",
      " -0.0086434   0.00161692]\n",
      "Weights on iteration  31 :  [0.73180393 0.35987986 0.08770899 0.52167511 0.50524904 0.5533402\n",
      " 0.72286858 0.51747431]\n",
      "Loss:  0.6267740864216826\n",
      "Weight derivatives:  [-0.01209435 -0.01965257  0.03585035  0.00755371 -0.00072847 -0.00403901\n",
      " -0.00852693  0.00163727]\n",
      "Weights on iteration  32 :  [0.7342228  0.36381037 0.08053892 0.52016437 0.50539473 0.554148\n",
      " 0.72457396 0.51714685]\n",
      "Loss:  0.6263701689501684\n",
      "Weight derivatives:  [-0.01198404 -0.01938138  0.03533649  0.00747203 -0.00062841 -0.00406187\n",
      " -0.00841175  0.00165893]\n",
      "Weights on iteration  33 :  [0.7366196  0.36768665 0.07347162 0.51866996 0.50552042 0.55496037\n",
      " 0.72625631 0.51681507]\n",
      "Loss:  0.6259770800734126\n",
      "Weight derivatives:  [-0.01187478 -0.01911245  0.03483259  0.00739174 -0.00053793 -0.00408306\n",
      " -0.00829782  0.00168173]\n",
      "Weights on iteration  34 :  [0.73899456 0.37150914 0.0665051  0.51719161 0.505628   0.55577699\n",
      " 0.72791588 0.51647872]\n",
      "Loss:  0.6255944891387119\n",
      "Weight derivatives:  [-0.01176658 -0.01884604  0.0343383   0.00731282 -0.00045615 -0.00410272\n",
      " -0.00818514  0.00170552]\n",
      "Weights on iteration  35 :  [0.74134788 0.37527835 0.05963744 0.51572905 0.50571923 0.55659753\n",
      " 0.72955291 0.51613762]\n",
      "Loss:  0.6252220772170404\n",
      "Weight derivatives:  [-0.01165946 -0.01858235  0.03385328  0.00723527 -0.00038227 -0.00412093\n",
      " -0.00807371  0.00173017]\n",
      "Weights on iteration  36 :  [0.74367977 0.37899482 0.05286679 0.51428199 0.50579569 0.55742171\n",
      " 0.73116765 0.51579158]\n",
      "Loss:  0.6248595364856685\n",
      "Weight derivatives:  [-0.01155343 -0.01832153  0.03337724  0.00715906 -0.00031555 -0.00413782\n",
      " -0.00796353  0.00175555]\n",
      "Weights on iteration  37 :  [0.74599045 0.38265912 0.04619134 0.51285018 0.5058588  0.55824928\n",
      " 0.73276035 0.51544047]\n",
      "Loss:  0.6245065696760305\n",
      "Weight derivatives:  [-0.01144851 -0.01806372  0.03290991  0.00708417 -0.00025532 -0.00415347\n",
      " -0.00785461  0.00178154]\n",
      "Weights on iteration  38 :  [0.74828016 0.38627187 0.03960936 0.51143335 0.50590986 0.55907997\n",
      " 0.73433128 0.51508417]\n",
      "Loss:  0.6241628895761998\n",
      "Weight derivatives:  [-0.01134474 -0.017809    0.03245102  0.00701059 -0.00020098 -0.00416797\n",
      " -0.00774696  0.00180805]\n",
      "Weights on iteration  39 :  [0.7505491  0.38983367 0.03311915 0.51003123 0.50595006 0.55991357\n",
      " 0.73588067 0.51472256]\n",
      "Loss:  0.6238282185793396\n",
      "Weight derivatives:  [-0.01124214 -0.01755746  0.03200033  0.00693828 -0.00015198 -0.00418141\n",
      " -0.0076406   0.00183498]\n",
      "Weights on iteration  40 :  [0.75279753 0.39334516 0.02671909 0.50864357 0.50598045 0.56074985\n",
      " 0.73740879 0.51435556]\n",
      "Loss:  0.623502288270982\n",
      "Weight derivatives:  [-0.01114071 -0.01730915  0.03155761  0.00686722 -0.0001078  -0.00419387\n",
      " -0.00753554  0.00186224]\n",
      "Weights on iteration  41 :  [0.75502567 0.39680699 0.02040757 0.50727013 0.50600201 0.56158862\n",
      " 0.7389159  0.51398311]\n",
      "Loss:  0.6231848390493451\n",
      "Weight derivatives:  [-1.10404847e-02 -1.70641071e-02  3.11226444e-02  6.79738884e-03\n",
      " -6.80080228e-05 -4.20541164e-03 -7.43178621e-03  1.88976437e-03]\n",
      "Weights on iteration  42 :  [0.75723377 0.40021981 0.01418304 0.50591065 0.50601561 0.5624297\n",
      " 0.74040225 0.51360516]\n",
      "Loss:  0.6228756197738418\n",
      "Weight derivatives:  [-1.09414693e-02 -1.68223705e-02  3.06952345e-02  6.72875031e-03\n",
      " -3.21739627e-05 -4.21610166e-03 -7.32934007e-03  1.91747064e-03]\n",
      "Weights on iteration  43 :  [0.75942207 0.40358428 0.00804399 0.5045649  0.50602205 0.56327293\n",
      " 0.74186812 0.51322166]\n",
      "Loss:  0.6225743874378465\n",
      "Weight derivatives:  [-1.08436763e-02 -1.65839556e-02  3.02751834e-02  6.66128251e-03\n",
      "  7.44864706e-08 -4.22600038e-03 -7.22820892e-03  1.94530084e-03]\n",
      "Weights on iteration  44 :  [0.7615908  0.40690107 0.00198895 0.50323264 0.50602203 0.56411813\n",
      " 0.74331376 0.5128326 ]\n",
      "Loss:  0.6222809068624463\n",
      "Weight derivatives:  [-1.07471137e-02 -1.63488722e-02  2.98623063e-02  6.59495983e-03\n",
      "  2.90787750e-05 -4.23516263e-03 -7.12839482e-03  1.97319848e-03]\n",
      "Weights on iteration  45 :  [ 0.76374022  0.41017085 -0.00398351  0.50191365  0.50601622  0.56496516\n",
      "  0.74473944  0.51243796]\n",
      "Loss:  0.6219949504084874\n",
      "Weight derivatives:  [-1.06517869e-02 -1.61171224e-02  2.94564272e-02  6.52975715e-03\n",
      "  5.51487417e-05 -4.24363888e-03 -7.02989776e-03  2.00111287e-03]\n",
      "Weights on iteration  46 :  [ 0.76587058  0.41339427 -0.00987479  0.5006077   0.50600519  0.56581389\n",
      "  0.74614542  0.51203774]\n",
      "Loss:  0.6217162977046643\n",
      "Weight derivatives:  [-1.05576989e-02 -1.58887013e-02  2.90573776e-02  6.46564990e-03\n",
      "  7.85654778e-05 -4.25147556e-03 -6.93271580e-03  2.02899856e-03]\n",
      "Weights on iteration  47 :  [ 0.76798212  0.41657201 -0.01568627  0.49931457  0.50598948  0.56666418\n",
      "  0.74753197  0.51163194]\n",
      "Loss:  0.6214447353898457\n",
      "Weight derivatives:  [-1.04648504e-02 -1.56635983e-02  2.86649963e-02  6.40261408e-03\n",
      "  9.95839415e-05 -4.25871539e-03 -6.83684519e-03  2.05681494e-03]\n",
      "Weights on iteration  48 :  [ 0.77007509  0.41970473 -0.02141927  0.49803405  0.50596956  0.56751592\n",
      "  0.74889934  0.51122058]\n",
      "Loss:  0.6211800568680546\n",
      "Weight derivatives:  [-0.01037324 -0.0154418   0.02827913  0.00634063  0.00011844 -0.0042654\n",
      " -0.00674228  0.00208453]\n",
      "Weights on iteration  49 :  [ 0.77214974  0.42279309 -0.02707509  0.49676592  0.50594587  0.568369\n",
      "  0.75024779  0.51080367]\n",
      "Loss:  0.6209220620748529\n",
      "Weight derivatives:  [-0.01028287 -0.01522328  0.02789963  0.00627966  0.00013533 -0.00427156\n",
      " -0.00664901  0.0021121 ]\n",
      "Weights on iteration  50 :  [ 0.77420631  0.42583775 -0.03265502  0.49550999  0.50591881  0.56922332\n",
      "  0.75157759  0.51038125]\n",
      "Loss:  0.6206705572540692\n",
      "Weight derivatives:  [-0.01019372 -0.01500802  0.02752635  0.0062197   0.00015046 -0.00427723\n",
      " -0.00655704  0.0021395 ]\n",
      "Weights on iteration  51 :  [ 0.77624506  0.42883935 -0.03816029  0.49426605  0.50588871  0.57007876\n",
      "  0.752889    0.50995335]\n",
      "Loss:  0.6204253547439441\n",
      "Weight derivatives:  [-0.0101058  -0.01479599  0.02715915  0.00616073  0.00016399 -0.00428245\n",
      " -0.00646635  0.00216672]\n",
      "Weights on iteration  52 :  [ 0.77826622  0.43179855 -0.04359212  0.4930339   0.50585592  0.57093525\n",
      "  0.75418227  0.50952001]\n",
      "Loss:  0.620186272771967\n",
      "Weight derivatives:  [-0.0100191  -0.01458716  0.02679791  0.00610271  0.00017608 -0.00428724\n",
      " -0.00637692  0.00219372]\n",
      "Weights on iteration  53 :  [ 0.78027004  0.43471598 -0.0489517   0.49181336  0.5058207   0.5717927\n",
      "  0.75545766  0.50908127]\n",
      "Loss:  0.6199531352577518\n",
      "Weight derivatives:  [-0.0099336  -0.0143815   0.02644249  0.00604563  0.00018687 -0.00429162\n",
      " -0.00628876  0.00222048]\n",
      "Weights on iteration  54 :  [ 0.78225676  0.43759228 -0.0542402   0.49060424  0.50578333  0.57265102\n",
      "  0.75671541  0.50863717]\n",
      "Loss:  0.6197257716234059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight derivatives:  [-0.0098493  -0.01417897  0.02609277  0.00598947  0.0001965  -0.00429563\n",
      " -0.00620184  0.002247  ]\n",
      "Weights on iteration  55 :  [ 0.78422662  0.44042807 -0.05945875  0.48940634  0.50574403  0.57351015\n",
      "  0.75795578  0.50818777]\n",
      "Loss:  0.6195040166109286\n",
      "Weight derivatives:  [-0.00976619 -0.01397953  0.02574863  0.00593421  0.00020507 -0.00429928\n",
      " -0.00611616  0.00227324]\n",
      "Weights on iteration  56 :  [ 0.78617985  0.44322398 -0.06460848  0.4882195   0.50570301  0.57437\n",
      "  0.75917901  0.50773312]\n",
      "Loss:  0.6192877101062251\n",
      "Weight derivatives:  [-0.00968426 -0.01378315  0.02540996  0.00587983  0.00021269 -0.00430259\n",
      " -0.0060317   0.00229921]\n",
      "Weights on iteration  57 :  [ 0.7881167   0.44598061 -0.06969047  0.48704353  0.50566047  0.57523052\n",
      "  0.76038535  0.50727328]\n",
      "Loss:  0.6190766969693943\n",
      "Weight derivatives:  [-0.00960348 -0.0135898   0.02507665  0.00582632  0.00021946 -0.00430559\n",
      " -0.00594844  0.00232488]\n",
      "Weights on iteration  58 :  [ 0.7900374   0.44869857 -0.0747058   0.48587827  0.50561658  0.57609164\n",
      "  0.76157504  0.5068083 ]\n",
      "Loss:  0.618870826870963\n",
      "Weight derivatives:  [-0.00952386 -0.01339941  0.02474858  0.00577365  0.00022546 -0.00430829\n",
      " -0.00586637  0.00235026]\n",
      "Weights on iteration  59 :  [ 0.79194217  0.45137845 -0.07965552  0.48472354  0.50557149  0.5769533\n",
      "  0.76274831  0.50633825]\n",
      "Loss:  0.618669954133805\n",
      "Weight derivatives:  [-0.00944538 -0.01321197  0.02442564  0.0057218   0.00023077 -0.00431071\n",
      " -0.00578548  0.00237532]\n",
      "Weights on iteration  60 :  [ 0.79383125  0.45402085 -0.08454064  0.48357918  0.50552533  0.57781544\n",
      "  0.76390541  0.50586319]\n",
      "Loss:  0.6184739375804852\n",
      "Weight derivatives:  [-0.00936802 -0.01302742  0.02410775  0.00567077  0.00023546 -0.00431286\n",
      " -0.00570575  0.00240007]\n",
      "Weights on iteration  61 :  [ 0.79570485  0.45662633 -0.08936219  0.48244503  0.50547824  0.57867801\n",
      "  0.76504656  0.50538317]\n",
      "Loss:  0.6182826403858006\n",
      "Weight derivatives:  [-0.00929176 -0.01284572  0.02379479  0.00562053  0.0002396  -0.00431476\n",
      " -0.00562717  0.00242449]\n",
      "Weights on iteration  62 :  [ 0.79756321  0.45919547 -0.09412115  0.48132092  0.50543032  0.57954096\n",
      "  0.76617199  0.50489827]\n",
      "Loss:  0.6180959299343274\n",
      "Weight derivatives:  [-0.00921661 -0.01266682  0.02348668  0.00557106  0.00024324 -0.00431642\n",
      " -0.00554971  0.00244859]\n",
      "Weights on iteration  63 :  [ 0.79940653  0.46172884 -0.09881849  0.48020671  0.50538167  0.58040425\n",
      "  0.76728193  0.50440856]\n",
      "Loss:  0.6179136776827702\n",
      "Weight derivatives:  [-0.00914253 -0.0124907   0.02318331  0.00552236  0.00024643 -0.00431786\n",
      " -0.00547337  0.00247236]\n",
      "Weights on iteration  64 :  [ 0.80123503  0.46422698 -0.10345515  0.47910224  0.50533239  0.58126782\n",
      "  0.76837661  0.50391408]\n",
      "Loss:  0.617735759026934\n",
      "Weight derivatives:  [-0.00906952 -0.01231729  0.0228846   0.0054744   0.00024922 -0.00431907\n",
      " -0.00539813  0.00249579]\n",
      "Weights on iteration  65 :  [ 0.80304894  0.46669043 -0.10803207  0.47800736  0.50528254  0.58213164\n",
      "  0.76945623  0.50341493]\n",
      "Loss:  0.6175620531731755\n",
      "Weight derivatives:  [-0.00899756 -0.01214656  0.02259046  0.00542717  0.00025165 -0.00432009\n",
      " -0.00532396  0.00251889]\n",
      "Weights on iteration  66 :  [ 0.80484845  0.46911975 -0.11255016  0.47692192  0.50523221  0.58299565\n",
      "  0.77052103  0.50291115]\n",
      "Loss:  0.617392443014147\n",
      "Weight derivatives:  [-0.00892663 -0.01197847  0.0223008   0.00538066  0.00025377 -0.0043209\n",
      " -0.00525087  0.00254165]\n",
      "Weights on iteration  67 :  [ 0.80663378  0.47151544 -0.11701032  0.47584579  0.50518146  0.58385983\n",
      "  0.7715712   0.50240282]\n",
      "Loss:  0.6172268150087084\n",
      "Weight derivatives:  [-0.00885673 -0.01181297  0.02201553  0.00533485  0.0002556  -0.00432154\n",
      " -0.00517882  0.00256407]\n",
      "Weights on iteration  68 :  [ 0.80840512  0.47387804 -0.12141343  0.47477882  0.50513034  0.58472414\n",
      "  0.77260696  0.50189001]\n",
      "Loss:  0.6170650590658658\n",
      "Weight derivatives:  [-0.00878783 -0.01165002  0.02173458  0.00528972  0.00025717 -0.00432199\n",
      " -0.0051078   0.00258616]\n",
      "Weights on iteration  69 :  [ 0.81016269  0.47620804 -0.12576035  0.47372088  0.50507891  0.58558854\n",
      "  0.77362852  0.50137277]\n",
      "Loss:  0.6169070684325809\n",
      "Weight derivatives:  [-0.00871992 -0.01148957  0.02145787  0.00524527  0.00025852 -0.00432228\n",
      " -0.0050378   0.0026079 ]\n",
      "Weights on iteration  70 :  [ 0.81190667  0.47850595 -0.13005192  0.47267182  0.5050272   0.58645299\n",
      "  0.77463608  0.50085119]\n",
      "Loss:  0.6167527395853507\n",
      "Weight derivatives:  [-0.00865298 -0.01133159  0.02118531  0.00520148  0.00025967 -0.0043224\n",
      " -0.0049688   0.00262931]\n",
      "Weights on iteration  71 :  [ 0.81363727  0.48077227 -0.13428898  0.47163153  0.50497527  0.58731747\n",
      "  0.77562984  0.50032533]\n",
      "Loss:  0.6166019721254131\n",
      "Weight derivatives:  [-0.008587   -0.01117603  0.02091683  0.00515834  0.00026065 -0.00432237\n",
      " -0.00490079  0.00265039]\n",
      "Weights on iteration  72 :  [ 0.81535467  0.48300748 -0.13847235  0.47059986  0.50492314  0.58818195\n",
      "  0.77661     0.49979525]\n",
      "Loss:  0.6164546686774701\n",
      "Weight derivatives:  [-0.00852197 -0.01102286  0.02065235  0.00511583  0.00026146 -0.00432219\n",
      " -0.00483374  0.00267113]\n",
      "Weights on iteration  73 :  [ 0.81705906  0.48521205 -0.14260282  0.46957669  0.50487085  0.58904639\n",
      "  0.77757675  0.49926103]\n",
      "Loss:  0.6163107347918114\n",
      "Weight derivatives:  [-0.00845787 -0.01087202  0.0203918   0.00507394  0.00026214 -0.00432188\n",
      " -0.00476765  0.00269154]\n",
      "Weights on iteration  74 :  [ 0.81875064  0.48738646 -0.14668118  0.4685619   0.50481842  0.58991076\n",
      "  0.77853028  0.49872272]\n",
      "Loss:  0.6161700788497139\n",
      "Weight derivatives:  [-0.00839469 -0.01072348  0.02013512  0.00503267  0.00026269 -0.00432142\n",
      " -0.0047025   0.00271162]\n",
      "Weights on iteration  75 :  [ 0.82042957  0.48953115 -0.1507082   0.46755537  0.50476588  0.59077505\n",
      "  0.77947078  0.4981804 ]\n",
      "Loss:  0.6160326119720331\n",
      "Weight derivatives:  [-0.0083324  -0.0105772   0.01988223  0.00499199  0.00026314 -0.00432084\n",
      " -0.00463827  0.00273137]\n",
      "Weights on iteration  76 :  [ 0.82209605  0.49164659 -0.15468465  0.46655697  0.50471325  0.59163922\n",
      "  0.78039843  0.49763412]\n",
      "Loss:  0.6158982479308619\n",
      "Weight derivatives:  [-0.008271   -0.01043315  0.01963306  0.00495189  0.00026349 -0.00432013\n",
      " -0.00457495  0.00275079]\n",
      "Weights on iteration  77 :  [ 0.82375026  0.49373322 -0.15861126  0.4655666   0.50466055  0.59250324\n",
      "  0.78131342  0.49708396]\n",
      "Loss:  0.6157669030641585\n",
      "Weight derivatives:  [-0.00821048 -0.01029127  0.01938754  0.00491237  0.00026376 -0.0043193\n",
      " -0.00451253  0.0027699 ]\n",
      "Weights on iteration  78 :  [ 0.82539235  0.49579148 -0.16248877  0.46458412  0.5046078   0.5933671\n",
      "  0.78221593  0.49652999]\n",
      "Loss:  0.6156384961932563\n",
      "Weight derivatives:  [-0.00815081 -0.01015154  0.01914562  0.00487342  0.00026396 -0.00431836\n",
      " -0.00445098  0.00278868]\n",
      "Weights on iteration  79 :  [ 0.82702251  0.49782178 -0.16631789  0.46360944  0.50455501  0.59423077\n",
      "  0.78310613  0.49597225]\n",
      "Loss:  0.615512948543148\n",
      "Weight derivatives:  [-0.00809198 -0.01001391  0.01890723  0.00483502  0.00026409 -0.00431731\n",
      " -0.0043903   0.00280715]\n",
      "Weights on iteration  80 :  [ 0.82864091  0.49982457 -0.17009934  0.46264243  0.50450219  0.59509424\n",
      "  0.78398419  0.49541082]\n",
      "Loss:  0.6153901836654526\n",
      "Weight derivatives:  [-0.00803399 -0.00987835  0.0186723   0.00479716  0.00026418 -0.00431615\n",
      " -0.00433046  0.00282531]\n",
      "Weights on iteration  81 :  [ 0.83024771  0.50180024 -0.1738338   0.461683    0.50444936  0.59595747\n",
      "  0.78485028  0.49484576]\n",
      "Loss:  0.615270127363984\n",
      "Weight derivatives:  [-0.00797681 -0.00974483  0.01844078  0.00475984  0.00026421 -0.00431489\n",
      " -0.00427147  0.00284315]\n",
      "Weights on iteration  82 :  [ 0.83184307  0.5037492  -0.17752196  0.46073103  0.50439651  0.59682044\n",
      "  0.78570457  0.49427713]\n",
      "Loss:  0.615152707622828\n",
      "Weight derivatives:  [-0.00792043 -0.00961329  0.01821261  0.00472303  0.00026421 -0.00431353\n",
      " -0.00421329  0.00286069]\n",
      "Weights on iteration  83 :  [ 0.83342715  0.50567186 -0.18116448  0.45978643  0.50434367  0.59768315\n",
      "  0.78654723  0.49370499]\n",
      "Loss:  0.6150378545368328\n",
      "Weight derivatives:  [-0.00786485 -0.00948373  0.01798772  0.00468674  0.00026417 -0.00431207\n",
      " -0.00415592  0.00287793]\n",
      "Weights on iteration  84 :  [ 0.83500012  0.50756861 -0.18476202  0.45884908  0.50429084  0.59854556\n",
      "  0.78737841  0.4931294 ]\n",
      "Loss:  0.6149255002444648\n",
      "Weight derivatives:  [-0.00781004 -0.00935608  0.01776606  0.00465096  0.00026411 -0.00431052\n",
      " -0.00409935  0.00289487]\n",
      "Weights on iteration  85 :  [ 0.83656213  0.50943982 -0.18831523  0.45791889  0.50423802  0.59940767\n",
      "  0.78819829  0.49255043]\n",
      "Loss:  0.6148155788628951\n",
      "Weight derivatives:  [-0.00775599 -0.00923033  0.01754758  0.00461566  0.00026402 -0.00430889\n",
      " -0.00404356  0.00291151]\n",
      "Weights on iteration  86 :  [ 0.83811333  0.51128589 -0.19182475  0.45699575  0.50418521  0.60026945\n",
      "  0.789007    0.49196812]\n",
      "Loss:  0.6147080264253061\n",
      "Weight derivatives:  [-0.0077027  -0.00910644  0.01733222  0.00458086  0.00026391 -0.00430717\n",
      " -0.00398854  0.00292787]\n",
      "Weights on iteration  87 :  [ 0.83965387  0.51310718 -0.19529119  0.45607958  0.50413243  0.60113088\n",
      "  0.78980471  0.49138255]\n",
      "Loss:  0.6146027808202856\n",
      "Weight derivatives:  [-0.00765015 -0.00898438  0.01711992  0.00454652  0.00026379 -0.00430537\n",
      " -0.00393428  0.00294393]\n",
      "Weights on iteration  88 :  [ 0.8411839   0.51490405 -0.19871518  0.45517028  0.50407967  0.60199195\n",
      "  0.79059156  0.49079377]\n",
      "Loss:  0.6144997817332638\n",
      "Weight derivatives:  [-0.00759832 -0.00886411  0.01691064  0.00451266  0.00026365 -0.00430348\n",
      " -0.00388076  0.00295971]\n",
      "Weights on iteration  89 :  [ 0.84270356  0.51667687 -0.20209731  0.45426775  0.50402694  0.60285265\n",
      "  0.79136771  0.49020182]\n",
      "Loss:  0.6143989705899348\n",
      "Weight derivatives:  [-0.00754721 -0.0087456   0.01670433  0.00447925  0.00026351 -0.00430152\n",
      " -0.00382797  0.00297521]\n",
      "Weights on iteration  90 :  [ 0.844213    0.51842599 -0.20543817  0.4533719   0.50397424  0.60371295\n",
      "  0.79213331  0.48960678]\n",
      "Loss:  0.6143002905015531\n",
      "Weight derivatives:  [-0.0074968  -0.00862883  0.01650092  0.00444629  0.00026336 -0.00429949\n",
      " -0.0037759   0.00299043]\n",
      "Weights on iteration  91 :  [ 0.84571236  0.52015176 -0.20873836  0.45248264  0.50392157  0.60457285\n",
      "  0.79288849  0.48900869]\n",
      "Loss:  0.6142036862120963\n",
      "Weight derivatives:  [-0.00744708 -0.00851375  0.01630038  0.00441378  0.0002632  -0.00429738\n",
      " -0.00372453  0.00300538]\n",
      "Weights on iteration  92 :  [ 0.84720178  0.52185451 -0.21199843  0.45159988  0.50386893  0.60543233\n",
      "  0.79363339  0.48840762]\n",
      "Loss:  0.6141091040471727\n",
      "Weight derivatives:  [-0.00739804 -0.00840035  0.01610266  0.00438169  0.00026304 -0.00429521\n",
      " -0.00367386  0.00302006]\n",
      "Weights on iteration  93 :  [ 0.84868138  0.52353458 -0.21521896  0.45072354  0.50381632  0.60629137\n",
      "  0.79436817  0.48780361]\n",
      "Loss:  0.6140164918646611\n",
      "Weight derivatives:  [-0.00734967 -0.00828859  0.0159077   0.00435004  0.00026288 -0.00429296\n",
      " -0.00362388  0.00303448]\n",
      "Weights on iteration  94 :  [ 0.85015132  0.5251923  -0.21840051  0.44985354  0.50376374  0.60714996\n",
      "  0.79509294  0.48719671]\n",
      "Loss:  0.613925799006993\n",
      "Weight derivatives:  [-0.00730196 -0.00817844  0.01571547  0.0043188   0.00026273 -0.00429065\n",
      " -0.00357457  0.00304863]\n",
      "Weights on iteration  95 :  [ 0.85161171  0.52682799 -0.2215436   0.44898978  0.50371119  0.60800809\n",
      "  0.79580786  0.48658699]\n",
      "Loss:  0.6138369762550229\n",
      "Weight derivatives:  [-0.00725489 -0.00806989  0.01552591  0.00428798  0.00026257 -0.00428828\n",
      " -0.00352592  0.00306252]\n",
      "Weights on iteration  96 :  [ 0.85306269  0.52844196 -0.22464878  0.44813218  0.50365868  0.60886575\n",
      "  0.79651304  0.48597448]\n",
      "Loss:  0.6137499757834595\n",
      "Weight derivatives:  [-0.00720847 -0.00796288  0.01533899  0.00425755  0.00026241 -0.00428585\n",
      " -0.00347792  0.00307616]\n",
      "Weights on iteration  97 :  [ 0.85450438  0.53003454 -0.22771658  0.44728067  0.5036062   0.60972292\n",
      "  0.79720862  0.48535925]\n",
      "Loss:  0.6136647511177611\n",
      "Weight derivatives:  [-0.00716266 -0.00785741  0.01515465  0.00422753  0.00026226 -0.00428335\n",
      " -0.00343056  0.00308955]\n",
      "Weights on iteration  98 :  [ 0.85593691  0.53160602 -0.23074751  0.44643516  0.50355375  0.61057959\n",
      "  0.79789473  0.48474134]\n",
      "Loss:  0.6135812570924752\n",
      "Weight derivatives:  [-0.00711748 -0.00775345  0.01497286  0.00419789  0.00026211 -0.0042808\n",
      " -0.00338383  0.00310269]\n",
      "Weights on iteration  99 :  [ 0.85736041  0.53315671 -0.23374208  0.44559558  0.50350132  0.61143575\n",
      "  0.7985715   0.4841208 ]\n",
      "Loss:  0.6134994498109619\n"
     ]
    }
   ],
   "source": [
    "weights = np.array([0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5])\n",
    "bias = 0.5\n",
    "\n",
    "def train_network(iterations, weights, bias):\n",
    "\n",
    "    # Alpha is step size (learning rate)\n",
    "    alpha = 0.2\n",
    "\n",
    "    # m is the amount of training data\n",
    "    m = training_data.shape[1]\n",
    "\n",
    "    for j in range(iterations):\n",
    "        \n",
    "        # l is the average of all the losses\n",
    "        l = 0.0\n",
    "        \n",
    "        # all derivitive weights and deriviative bias start at 0\n",
    "        derivative_weights = np.zeros(training_data.shape[0])\n",
    "        db = 0.0\n",
    "\n",
    "        for i in range(m):\n",
    "            # grab features\n",
    "            features = training_data[0:8, i]\n",
    "            # make prediction\n",
    "            yhat = sigmoid(features, weights, bias)\n",
    "            # get actual value\n",
    "            y = raw_training_data[8, i]\n",
    "            # calculate the loss\n",
    "            l += loss(y, yhat)\n",
    "            # find the derivative of the activation function\n",
    "            dz = dz_calc(yhat, y)\n",
    "            # calculate the deriviative of each weight and the bias\n",
    "            for k in range(len(derivative_weights)):\n",
    "                derivative_weights[k] += features[k] * dz\n",
    "            db += dz\n",
    "\n",
    "        # Calculate average for loss, derivarive bias, and all derivative weights\n",
    "        l /= m\n",
    "        for i in range(len(derivative_weights)):\n",
    "            derivative_weights[i] /= m\n",
    "        db /= m\n",
    "        \n",
    "        store_loss(l, j)\n",
    "\n",
    "        # Update weights with the derivative weight by learning rate\n",
    "        for i in range(len(weights)):\n",
    "            weights[i] = weights[i]-alpha*derivative_weights[i]\n",
    "        bias = bias - alpha*db\n",
    "        \n",
    "        \n",
    "\n",
    "        if j%1 == 0:\n",
    "            print(\"Weight derivatives: \", derivative_weights)\n",
    "            print(\"Weights on iteration \", j, \": \", weights)\n",
    "            print(\"Loss: \", l)\n",
    "        \n",
    "        elif j == iterations-1:\n",
    "            print(\"Weight derivatives: \", derivative_weights)\n",
    "            print(\"Weights on iteration \", j+1, \": \", weights)\n",
    "        \n",
    "\n",
    "    # clear loss_vec && iterations_vec\n",
    "train_network(100, weights, bias)\n",
    "last_train = weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good predictions:  182\n",
      "Total predictions:  268\n",
      "67.91044776119402 %\n"
     ]
    }
   ],
   "source": [
    "def test_weights(weights, bias):\n",
    "    m = testing_data.shape[1]\n",
    "    good_pred = 0\n",
    "\n",
    "    for i in range(m):\n",
    "        features = testing_data[0:8, i]\n",
    "        # make prediction\n",
    "        yhat = sigmoid(features, weights, bias)\n",
    "        #print(\"Prediction\", yhat)\n",
    "        # get actual value\n",
    "        y = raw_testing_data[8, i]\n",
    "        #print(\"Actual\", y)\n",
    "\n",
    "        #if np.isclose(y, yhat):\n",
    "        #    print(\"TRUE\")\n",
    "        #    good_pred += 1\n",
    "        \n",
    "        if yhat < 0.5 and y == 0.0:\n",
    "            good_pred += 1\n",
    "        elif yhat >= 0.5 and y == 1.0:\n",
    "            good_pred += 1\n",
    "\n",
    "    print(\"Good predictions: \", good_pred)\n",
    "    print(\"Total predictions: \", m)\n",
    "    \n",
    "    print(good_pred/m*100, \"%\")\n",
    "        \n",
    "test_weights(weights, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x249820061c0>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD7CAYAAABjVUMJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAA/wklEQVR4nO3dd3xUVf7/8ddnajLpCSGUhI50QYkoIkQXULBhWV2wrmXtFX7uV7e6363fXcGy9raW3dW1gx3BNRRBDUrvnVBSSe/J+f0xN8kkJGSAJJNkPs/HI4+Ze+beybkPyLznlHuuGGNQSikVfGyBroBSSqnA0ABQSqkgpQGglFJBSgNAKaWClAaAUkoFKQ0ApZQKUn4FgIhME5EtIrJdRB5s4vUoEflQRNaIyAYRuaGlY0UkVkS+EJFt1mNM65ySUkopf7QYACJiB54CpgPDgVkiMrzRbncCG40xo4Gzgbki4mrh2AeBxcaYwcBia1sppVQ7cfixzzhguzFmJ4CIvAnMADb67GOACBERIBzIBaqA049y7Ay8YQHwKvAV8D9Hq0i3bt1Mv379/KiyUkqpWqtWrco2xsQ3LvcnAHoD+3y20/F+sPt6ElgAHAAigJ8YY2pE5GjHJhhjDgIYYw6KSPemfrmI3ALcAtCnTx/S0tL8qLJSSqlaIrKnqXJ/xgCkibLG60ecB6wGegFjgCdFJNLPY4/KGPO8MSbZGJMcH39EgCmllDpO/gRAOpDks52I95u+rxuA94zXdmAXMLSFYzNEpCeA9Zh57NVXSil1vPwJgO+AwSLSX0RcwEy83T2+9gKTAUQkARgC7Gzh2AXA9dbz64H5J3IiSimljk2LYwDGmCoRuQv4HLADLxtjNojIbdbrzwK/B14RkXV4u33+xxiTDdDUsdZb/wV4S0RuwhsgV7TuqSmllDoa6UzLQScnJxsdBFZKqWMjIquMMcmNy/VKYKWUClIaAEopFaSCIgBeWb6L8x9fSlV1TaCropRSHUZQBMDDH25k48ECZjy1nLXpeYGujlJKdQhBEQC1MgrKueSp5Ty8YAOFZZWBro5SSgVUUARAmMsOwAd3nsk1Z/Tl1RW7mTIvlU/XHaQzzYJSSqnWFBQBkBAZAkBZZTX/O2Mk798xgdgwN7f/63tuejWNfbklAa6hUkq1v6AIgPgINwCZBeUAjEmK5sO7JvCrC4axYkcO5z66hOdSd1Cpg8RKqSASFAHQ3WoBZBSW1ZU57DZunjiARXNSmDAojj9/upmL/r6M7/ceDlQ1lVKqXQVFACQ0agH46h0dygvXJfPsNWPJK6nk8me+5lcfrCO/VAeJlVJdW1AEQPdIKwAKjwwAABFh2sgeLJqTwk/P7Me/v9nLlHmpfLjmgA4SK6W6rKAIgNpB4IyCsqPuF+528NuLRjD/zrPoERnC3W/8wPX/+I69OTpIrJTqeoIiAOLCjt4CaGxUYhQf3DmB3140nFW7c5n6aCpP/Xc7FVU6SKyU6jqCIgBCXd7TzGyhBeDLbhNumNCfRXNSOGdId/72+RYu/PtSvtud21bVVEqpdhUUAeB2eC8E87cF4KtnVCjPXjuWF69Lpri8miueXcGD764lr6SitauplFLtKigCIMTpDYCSiurjfo8pwxNYeP8kbpk0gLdXpTN5birv/5Cug8RKqU4rSAKgdU4zzO3gF+cP48O7ziIp1sP9/1nDNS99w86solZ5f6WUak9BEgD2Vn2/4b0ieff2M/n9jBGs3ZfPtMeX8viibZRXHX8LQyml2psGwHGy24Rrx/dj8ZwUzh2ewKOLtjL98aWs2JHT6r9LKaXagl8BICLTRGSLiGwXkQebeP0BEVlt/awXkWoRiRWRIT7lq0WkQETus455WET2+7x2fiufW50QR9vlXPfIEJ686lReueE0KqtrmPXCSua8tYbcYh0kVkp1bC1+MoqIHXgKmA4MB2aJyHDffYwxfzPGjDHGjAEeAlKNMbnGmC0+5WOBEuB9n0MfrX3dGPNJ65zSkRz2+tNsq26as4d0Z+F9Kdxx9kDmr97P5Llf8VbaPh0kVkp1WP58NR4HbDfG7DTGVABvAjOOsv8s4I0myicDO4wxe469mq2nsKyqzd471GXn59OG8vE9ExkYH87P31nLzOdXsj1TB4mVUh2PPwHQG9jns51ulR1BRDzANODdJl6eyZHBcJeIrBWRl0Ukppn3vEVE0kQkLSsry4/qHl17LPI2pEcEb906nj9fNopNBwuY/vgS5i3cQlmlDhIrpToOfwJAmihrrl/jImC5MabB5bIi4gIuBt72KX4GGAiMAQ4Cc5t6Q2PM88aYZGNMcnx8vB/VPbqCdlrl02YTZo3rw+I5Z3PBqJ488eV2pj22hGXbstvl9yulVEv8CYB0IMlnOxE40My+TX3LB+/4wffGmIzaAmNMhjGm2hhTA7yAt6upzbX3Ms/xEW4em3kK/7zpdACueekb7nvzB7KLjv2qZKWUak3+BMB3wGAR6W99k58JLGi8k4hEASnA/Cbe44hxARHp6bN5KbDe30ofD5c1E6igDccAjuaswd347L5J3POjQXy87iA/euQr3vh2LzU1OkislAqMFgPAGFMF3AV8DmwC3jLGbBCR20TkNp9dLwUWGmOKfY+3xgWmAu81euu/isg6EVkLnAPcfwLn0aIE654A7dUF1JQQp53Z5w7h03snMrRnJA+9t44rn1vBlkOFAauTUip4SWeappicnGzS0tKO69jLn/maVXsO88B5Q7jznEGtXLNjZ4zhnVXp/OmTTRSWVfGzSQO450eDCXW1/kVrSqngJiKrjDHJjcuD4kpggKhQJwAFZR3jVo8iwhXJSSyeczaXnNKbZ77awbmPpfLVlsxAV00pFSSCJgBqF4QLZBdQU2LDXDxyxWje+NkZOO02fvqP77jr398f070LlFLqeARPAFj3BCgoDcwgcEvGD4zj03sncv+Uk1i4MYPJc1N5feUeHSRWSrWZoAkAt7UgXHtPAz0Wboede6cM5rN7JzIqMYpff7Cey575mo0HCgJdNaVUFxQ0AVDXBdRBxgCOZkB8OP+6+XQe/clo9uWWcNGTy/jjxxspLu+YrRelVOcURAFQ2wXU8QMAvIPEl56SyOI5KVwxNpEXlu7i3EeXsGhjRssHK6WUH4InAKwxgJxOtkxztMfFXy4/mbdvG0+Y287Nr6Vx2+urOJhfGuiqKaU6ueAJAKsLqLCsqlMu0Xxav1g+unsiD5w3hP9uyWTK3FT+sXwX1TpIrJQ6TkEUAPUXWBWfwM3hA8nlsHHnOYP44v4UxvaL5XcfbuSSp5azLj0/0FVTSnVCQRQA9afaWcYBmtMnzsOrN5zG32edwqGCMmY8tYzffbiBIh0kVkodgyAKgPoWQGeYCdQSEeGi0b1YNDuFq07vwytf72bK3FQ+W3+oU3ZxKaXaX9AEgNtRHwD5JZ0/AGpFhTr5wyWjePf2M4n2OLntn6v42Wtp7M/TQWKl1NEFTQA06AIK0JLQbenUPjF8ePdZ/OL8oSzfnsPUeam8sGQnVdU1ga6aUqqDCqIA8GkBdPIxgOY47TZumTSQL2ZP4owBcfzxk01c/ORyVu/LC3TVlFIdUFAGQGcfBG5JYoyHl65P5pmrTyWnuJxLn17Ob+av7xJjH0qp1hNEAeDbBdT1PwhFhOmjerJodgrXj+/H6yv3MGVuKh+vPaiDxEopIJgCwNH1u4CaEhHi5OGLRzD/zgl0j3Rz57+/54ZXvmNfbkmgq6aUCrDgCYAGXUBdbxC4JScnRvPBHRP49YXD+W5XLlMfTeWZr3ZQqYPESgWtIAqA+lMNphaAL4fdxk1n9eeL2SlMGhzP/322mQufWMaqPbmBrppSKgD8CgARmSYiW0Rku4g82MTrD4jIautnvYhUi0is9dpu6+bvq0UkzeeYWBH5QkS2WY8xrXdaR/K9DiAYxgCOpld0KM9fl8zz146lsKySy59ZwUPvretS10copVrWYgCIiB14CpgODAdmichw332MMX8zxowxxowBHgJSjTG+XyvPsV73vSnxg8BiY8xgYLG13Wbcjq6zFERrOXdED76YncLNZ/XnP9/tZfK8r5i/er8OEisVJPxpAYwDthtjdhpjKoA3gRlH2X8W8IYf7zsDeNV6/ipwiR/HHDebTXA5OuZ9gQMpzO3gVxcOZ8FdZ9E7OpR731zNdS9/y+7s4kBXTSnVxvwJgN7APp/tdKvsCCLiAaYB7/oUG2ChiKwSkVt8yhOMMQcBrMfuzbznLSKSJiJpWVlZflS3eSG1AdAFrwQ+USN7R/HeHRP43xkj+GFvHuc+toS/L95GeVXnXDlVKdUyfwJAmihrro/gImB5o+6fCcaYU/F2Id0pIpOOpYLGmOeNMcnGmOT4+PhjOfQItTOBisqrqKjS2S+N2W3CdeP7sXhOClOHJTD3i62c//hSvtmZE+iqKaXagD8BkA4k+WwnAgea2Xcmjbp/jDEHrMdM4H28XUoAGSLSE8B6zPS/2sfHdypoTnF5W/+6TishMoSnrj6Vf/z0NMqravjJ8yt54O01HO5kd1NTSh2dPwHwHTBYRPqLiAvvh/yCxjuJSBSQAsz3KQsTkYja58C5wHrr5QXA9dbz632PayshThs2qz2TXagfZi05Z2h3vrg/hdtSBvL+D/uZPC+Vd1al6yCxUl1EiwFgjKkC7gI+BzYBbxljNojIbSJym8+ulwILjTG+o4cJwDIRWQN8C3xsjPnMeu0vwFQR2QZMtbbbVIjTTrjbAUB2kbYA/BHqsvPg9KF8dM9Z9Ivz8P/eXsOsF1ayI6so0FVTSp0g6Uzf5pKTk01aWlrLOzbjymdXsD+vlP15pfz1xydzZXJSywepOjU1hje+28v/fbqZssoabj97ILefPbBB15pSquMRkVWNpuEDQXQlMIDbadMWwAmw2YSrT+/L4jlnM31UDx5fvI3pjy/l6+3Zga6aUuo4BFUAhDjtiECYy65jACcgPsLN4zNP4bUbx1FjDFe9+A2z/7OaHA1VpTqVoAuAsspqukW4tQXQCiadFM/n903irnMG8eHaA/xobir/+W4vNTWdp1tRqWAWXAHgsFFWWUO3cA2A1hLitPP/zhvCJ/dMZEhCBP/z7jp+8vwKtmUUBrpqSqkWBFcAOO2UVVXTLdylAdDKBidE8OYtZ/DXy09mW2YR5z+xlL99vpmySr2SWKmOKsgCwObtAgp3k12kYwCtzWYTrjwticWzU7hodC+e+u8Ozn10CUu2ntgSHkqpthFkAWCnrLKGuHA3h0sqqNKbobSJuHA3864cw79vPh2HTbju5W+5540fyCwsC3TVlFI+gi4AAKJCnRgDubq0QZs6c1A3Prl3IvdNGcxn6w8xeW4q/1y5RweJleoggioAau8JEGFdC5Cl4wBtLsRp574pJ/HpfRMZ2SuKX32wnh8/+zWbDxUEumpKBb2gCoDaFkB4SO3FYNoCaC8D48P5989OZ+4Vo9mdU8IFTyzjz59soqRCl+ZWKlCCMwBqrwYu1BZAexIRLh+byOLZKVx+am+eW7KTqfOW8OXmjEBXTamgFGQB4D3d2haAdgEFRkyYi7/+eDRv3TqeUJedG19J445/rSKjQAeJlWpPwRUA1o3h7SKEOG3aAgiwcf1j+eSeiTxw3hAWb8pk8txUXlm+i2odJFaqXQRXAFhdQOVVejVwR+Fy2LjznEEsvH8Sp/SJ5uEPN3Lp08tZvz8/0FVTqssLsgDwnq5eDNbx9I0L47Ubx/H4zDEcyCvj4ieX8fuPNlJcroPESrWVIAsAbwugPgC0BdCRiAgzxvRm8ewUZo7rw0vLdjFlXioLNxwKdNWU6pKCLACsFkBVDfERuh5QRxXlcfKnS0fx7u3jiQxxcsvrq/jZa2kcyCsNdNWU6lKCKgDcjoYtgNziCh1w7MDG9o3lo3vO4sHpQ1m6LYsp81J5celOXcJDqVbiVwCIyDQR2SIi20XkwSZef0BEVls/60WkWkRiRSRJRP4rIptEZIOI3OtzzMMist/nuPNb88SaUjcIbAVAjS4H0eE57TZuSxnIF/encHr/WP7w8SZmPLWcNfvyAl01pTq9FgNAROzAU8B0YDgwS0SG++5jjPmbMWaMMWYM8BCQaozJBaqAOcaYYcAZwJ2Njn209jhjzCetc0rNc9cNAntnAYHeGrKzSIr18PJPT+Ppq08lq7CcS55ezm/nr6ewrDLQVVOq0/KnBTAO2G6M2WmMqQDeBGYcZf9ZwBsAxpiDxpjvreeFwCag94lV+fiFNOgCcgEaAJ2JiHD+qJ4smpPCdWf05bWVe5gyL5VP1h3EGO3KU+pY+RMAvYF9PtvpNPMhLiIeYBrwbhOv9QNOAb7xKb5LRNaKyMsiEtPMe94iImkikpaVdWLryjvtgk3w3hQmQlsAnVVkiJPfzRjJ+3dMIC7MzR3/+p6bXk1jX25JoKumVKfiTwBIE2XNfd26CFhudf/Uv4FION5QuM8YU7sM5DPAQGAMcBCY29QbGmOeN8YkG2OS4+Pj/ahu80Sk7p4AdV1AenP4TmtMUjQL7prAry4YxsqdOZz76BKeS91BpQ4SK+UXfwIgHUjy2U4EDjSz70ys7p9aIuLE++H/L2PMe7XlxpgMY0y1MaYGeAFvV1Obq70xfGSIA5fdpi2ATs5ht3HzxAF8MTuFCYO68edPN3PR35exas/hQFdNqQ7PnwD4DhgsIv1FxIX3Q35B451EJApIAeb7lAnwErDJGDOv0f49fTYvBdYfe/WPXe2N4UWEbuEuXRCui+gdHcqL1yfz3LVjySup5MfPfs0v319HfqkOEivVHEdLOxhjqkTkLuBzwA68bIzZICK3Wa8/a+16KbDQGFPsc/gE4FpgnYistsp+Yc34+auIjMHbnbQbuPXET6dltTeGB+gWoctBdDXnjejBhEHdmLdwK698vYvPN2Tw6wuHcfHoXni/jyilarUYAADWB/YnjcqebbT9CvBKo7JlND2GgDHm2mOoZ6txO+2UV1oBEO7mUL4uQdzVhLsd/Oai4Vx2am9+8f467n1zNe+sSucPl4ykb1xYoKunVIcRVFcCg3c5iLJK7yBht3BdDqIrG9k7ivfvmMDDFw3nh715nPvoEp7673YqqnSQWCkIxgBweAeBwdsCyCmu0JuUd2F2m/DTCf1ZNDuFHw3tzt8+38IFTyzl2125LR+sVBcXfAHgtNWNAXSPcFNdY8jR5SC6vB5RITxzzVheuj6ZkopqrnxuBf/zzlrySvTfXgWvIAwAe10XUO8YDwD7dZXJoDF5WAJfzJ7ErZMG8M736Uyem8p736frlcQqKAVpAHhbAEmxoQB6BWmQ8bgcPHT+MD686yySYj3MfmsNV7/4DTuzigJdNaXaVRAGQP0gcKLVAth3WAMgGA3vFcl7t5/JHy4Zybr9+Ux7bCmPLdpKudVFqFRXF3QB4HbUTwMNdzuI8ThJP6xdQMHKZhOuOaMvi+ekcN7IHjy2aBvTH1/Kih05ga6aUm0u6ALA90Iw8C4zrF1AqntECH+fdQqv3jiOyuoaZr2wkjlvrdH7RaguLegCINxtp7La1I8DxHjYry0AZUk5KZ6F96Vwx9kDmb96Pz+a+xVvpe3TQWLVJQVdAMSEee8DkFfiXSMmMSaU9MOlei2AqhPqsvPzaUP55N6JDIoP5+fvrOUnz69ke2ZhoKumVKsKvgDweAPgsDX/OzHWQ0V1DZmFekWwauikhAjeunU8f7lsFFsOFTL98aXMXbilrvWoVGcXdAEQ7XECPgEQ450Kmq4zgVQTbDZh5rg+LJ6TwoUn9+LvX25n2mNLWLYtO9BVU+qEBV0A1LUAir1dQEk6FVT5oVu4m0d/MoZ/3nQ6ANe89A33vvkDWdpyVJ1Y0AVAbFijLqCY2ovBdCBYteyswd347L5J3DN5MJ+sO8jkuV/x72/26hiS6pSCLgBqu4Bq14AJcdqJj3BrF5DyW4jTzuypJ/HpvZMY1jOSX7y/jiueW8HmQwUtH6xUBxJ0AeB22PG47Bwuqb9TVFJMqLYA1DEb1D2cN285g0euGM3OrCIufGIZf/l0M6UVOkisOoegCwDwjgMc9rnAJynWQ3qetgDUsRMRfjw2kcVzzubSU3rzbOoOpj6ayn+3ZAa6akq1KCgDINrjrBsDAO84wIG8Mqqq9UYh6vjEhrn42xWjefOWM3A7bNzwj++481/fk1Ggd5xTHVdQBkBsmKtRF5CH6hrDQb09pDpBZwyI45N7JzJn6kl8sSmDKXNTeW3Fbqp1kFh1QH4FgIhME5EtIrJdRB5s4vUHRGS19bNeRKpFJPZox4pIrIh8ISLbrMeY1juto4v2uBrcCKR2VVBdFE61BrfDzt2TB/P5fZMYnRTNb+Zv4LJnvmbDgfxAV02pBloMABGxA08B04HhwCwRGe67jzHmb8aYMcaYMcBDQKoxJreFYx8EFhtjBgOLre12EeNxNmwB1N4XQGcCqVbUv1sYr980jsd+Mob9h0u4+Mnl/PHjjRSXVwW6akoB/rUAxgHbjTE7jTEVwJvAjKPsPwt4w49jZwCvWs9fBS45xroft2iPi/zSyro+/55RodgE0nVVUNXKRIRLTunNotkpXJmcyAtLdzF1XiqLNmYEumpK+RUAvYF9PtvpVtkRRMQDTAPe9ePYBGPMQQDrsXsz73mLiKSJSFpWVpYf1W1ZrHUtQH6ptxXgctjoERmiXUCqzUR7XPz5spN557bxhIc4uPm1NG59PY2D+fp/TgWOPwEgTZQ1N6J1EbDcGJN7HMc2yRjzvDEm2RiTHB8ffyyHNium7mrg+m6gxFiPdgGpNpfcL5aP7p7Iz6cNIXVrFlPmpvLysl06SKwCwp8ASAeSfLYTgQPN7DuT+u6flo7NEJGeANZju02cjvbULgndcCqotgBUe3A5bNxx9iAW3pdCcr9Y/vejjcx4ahnr0nWQWLUvfwLgO2CwiPQXERfeD/kFjXcSkSggBZjv57ELgOut59c3Oq5NxdStCNpwKuihgjK9H6xqN33iPLxyw2k8edUpZBSUM+OpZTy8YAOFZZUtH6xUK2gxAIwxVcBdwOfAJuAtY8wGEblNRG7z2fVSYKExprilY62X/wJMFZFtwFRru13Urwha3wLoE+vBGF0UTrUvEeHCk3uxaHYKV5/el1dX7GbqvCV8tv6g3oVMtTmHPzsZYz4BPmlU9myj7VeAV/w51irPASb7X9XWE9NoRVDw3vwDYGtGIYO6hweiWiqIRYU6+f0lI7ns1N489N46bvvn90wZ1p2HLx5Rd52KUq0tKK8EDnPZcdqlQRfQ4IRwbAKbD+qKjipwTukTw4d3n8Uvzh/K8u05TJ23hBeW7NRlSlSbCMoAEJEjrgYOcdoZEB/OpkN631cVWE67jVsmDeSL2ZM4c2Acf/xkExc9uZwf9h4OdNVUFxOUAQC1VwNXNCgb2iOCTdoCUB1EYoyHF69P5tlrTuVwcQWXPfM1v/5gPQU6SKxaSdAGQLTHVXdbyFrDekaSfrhU/8BUhyEiTBvZk0VzUvjpmf341zd7mDw3lQ/XHNBBYnXCgjYAYj2uJlsAAFu1G0h1MOFuB7+9aATz7zyLhEg3d7/xAz/9x3fszdGLF9XxC9oAiAlruCAceFsAgI4DqA5rVGIUH9wxgd9cOJy03blMfTSVp7/aTqUOEqvjELQBUDsI7NuM7hkVQmSIQ8cBVIfmsNu48az+LJqTwtlD4vnrZ1u44ImlpO3ObflgpXwEbQDEeJxU1RgKfZbmFRGG9ozUqaCqU+gZFcpz1ybz4nXJFJdX8+NnV/DQe2sbzG5T6miCOACs9YAaDwT3iGDLoUJqdHEu1UlMGZ7Awvsn8bOJ/XkrLZ3Jc1P54If9OkisWhT0AXDEQHDPSIorqnVhONWphLkd/PKC4Sy4awKJsR7u+89qrn3pW3ZlF7d8sApawRsAYbULwjUMgNqB4I3aDaQ6oRG9onjv9jP5/YwRrNmXx3mPLeGJxdt0kUPVpKANgPoloRt2AZ2UEI4IbD6kAaA6J7tNuHZ8PxbNSWHq8ATmfbGV8x9fysqdOYGumupggjYAaruAcosbtgA8Lgf94sLYfFCngqrOLSEyhKeuOpV/3HAa5VU1zHx+JQ+8veaI//MqeAVtAESFOhGhyRkTQ3tEaAtAdRnnDOnOF/encFvKQN7/YT+T537FO6vSdZBYBW8A2G1CVOiRF4OBdxxgT24JxT5TRJXqzEJddh6cPpSP7jmLAfHh/L+31zDrhZVszywKdNVUAAVtAIC3G6jxIDB4A8AYHQhWXc/QHpG8fet4/nTpKDYeKOD8x5cy74utlFXqIHEwCuoAiPY4jxgEBhjbNwaAlTt00Ex1PTabcNXpfVg852ymj+rBE4u3Mf3xpSzfnh3oqql2FtQBEOtxNTkgFhvmYmiPCFborAnVhcVHuHl85im8ftM4aozh6he/4f7/rCa7qDzQVVPtJKgDoPFNYXydObAbq/Yc1vnTqsubODiez++bxN0/GsRHaw8weW4qb367V6+GDwJ+BYCITBORLSKyXUQebGafs0VktYhsEJFUq2yIVVb7UyAi91mvPSwi+31eO7/VzspP3pvCNL32//iBcZRX1fDD3rz2rZRSARDitDPn3CF8eu9EhvSI4MH31vGT51ewNUOnQ3dlLQaAiNiBp4DpwHBglogMb7RPNPA0cLExZgRwBYAxZosxZowxZgwwFigB3vc59NHa162bx7ermDAXpZXVTQ6Ajesfi03gax0HUEFkUPcI/nPLGfz1xyezLbOI8x9fyl8/20xphbaEuyJ/WgDjgO3GmJ3GmArgTWBGo32uAt4zxuwFMMZkNvE+k4Edxpg9J1Lh1tQzKgSA9MNH3lQjKtTJyN5ROhCsgo6IcGVyEotnpzBjTG+e/moH5z6WyldbmvqzVp2ZPwHQG9jns51ulfk6CYgRka9EZJWIXNfE+8wE3mhUdpeIrBWRl0UkpqlfLiK3iEiaiKRlZWX5UV3/DeoeDtDsXOjxA+L4Yd9h/fajglJcuJu5V47m3z87HafNxk//8R13/ft7MgvKAl011Ur8CQBpoqzx6JADbxfPBcB5wK9F5KS6NxBxARcDb/sc8wwwEBgDHATmNvXLjTHPG2OSjTHJ8fHxflTXfwPjjx4AZwyMo7LakLZHb7ShgteZA7vx6X0TuX/KSSzckMHkeam8vnKPDhJ3Af4EQDqQ5LOdCBxoYp/PjDHFxphsYAkw2uf16cD3xpiM2gJjTIYxptoYUwO8gLerqV2FuR30jg5lWzMBcFq/WBw2YYV2A6kg53bYuXfKYD67byKjekfx6w/Wc/mzX+vd8zo5fwLgO2CwiPS3vsnPBBY02mc+MFFEHCLiAU4HNvm8PotG3T8i0tNn81Jg/bFWvjUM7B7ebAsg3O3g5MQovR5AKcuA+HD+dfPpzLtyNHtySrjw78v48yebKKnQZVM6oxYDwBhTBdwFfI73Q/0tY8wGEblNRG6z9tkEfAasBb4FXjTGrAewAmEq8F6jt/6riKwTkbXAOcD9rXROx2RQfDg7soqabc6OHxjH2vR8inRdIKUA7yDxZacm8uWcFK4Ym8hzS3Yydd4SFm/KaPlg1aH4dR2AMeYTY8xJxpiBxpg/WmXPGmOe9dnnb8aY4caYkcaYx3zKS4wxccaY/Ebvea0xZpQx5mRjzMXGmIOtdE7HZHBCOGWVNezPa/oOYGcO7EZ1jeFrvUxeqQaiPS7+cvnJvHXreDwuOze9msbt/1zFoXwdJO4sgvpKYPCZCZTVdDfQuP6xxHicLFjTeNhDKQXev5GP75nIA+cN4cvNmUyZl8ory3dRrYPEHZ4GQO1MoIymA8Bpt3HR6F58sTGDwrKmrxpWKti5HDbuPGcQC++fxCl9onn4w41c+vRy1u/Pb/lgFTBBHwAxYS7iwlxHXRd9xpjelFfV8Nn6Q+1YM6U6n75xYbx24ziemHUKB/LKuPjJZfzvhxt1DK2DCvoAAG83UHNdQACn9ommT6yH+au1G0iplogIF4/uxeI5Kcwa14eXl+9i6rxUPt+gX6A6Gg0ArADILGr2FnkiwiWn9Gb5jmwy9CpIpfwSFerkj5eO4t3bzyQq1Mmtr6/iZ6+lcaCZCReq/WkA4A2A/NJKsouav1n2JWN6YQws0FaAUsdkbN8YPrz7LB6aPpSl27KYMi+VF5fupKq6JtBVC3oaANTPBNqW2fzStwPiwxmdFM0Hq/e3V7WU6jKcdhu3pgzki/tTOGNAHH/4eBMXP7mcNfvyAl21oKYBAAzuHgHAjhZukH3JmF5sOFCga6QrdZySYj28dH0yz1x9KjnF5Vzy9HJ+M389BTrDLiA0AICESDfhbsdRZwIBXDS6Fy6HjZeX7WqnminV9YgI00f1ZNHsFK4f34/XV+5hytxUPl57sNlxONU2NADw/occ2MJMIIBu4W5mnZbEO6vS2Zd75D0ElFL+iwhx8vDFI/jgjgnER7i589/fc+Mr3+nfVjvSALAMig9nWzMXg/m6/exB2ER4+qvt7VArpbq+0UnRzL9zAr+6YBjf7Mpl6qOpPJu6g0odJG5zGgCWwQnhZBaWk1969L7IHlEhzByXxNtp2gpQqrU47DZunjiARbNTmDQ4nr98upmL/r6MVXsOB7pqXZoGgGV0YjSAX4u+3X72QKsVsKONa6VUcOkVHcrz1yXz/LVjyS+t5PJnvuYX768jv0QHiduCBoDltH4xxIW5+Hhdy4uS9owK5SenJfF22r4m7yeslDox547owRezU7jprP68+e1eJs/7ivmr9+sgcSvTALA47DbOHdGDLzdnUlbZ8j2Abz97IHab8PCCjfqfUqk2EO528OsLh7PgrrPoFR3KvW+u5rqXv2VPTnGgq9ZlaAD4uGBUT0oqqvlqS2aL+/aKDuWB84awaFMG76xKb4faKRWcRvaO4v07JvC7i0fww948zn10CU9+uY2KKh0kPlEaAD7OGBBLbJiLT9b5t2jVjRP6c3r/WH734UbtClKqDdltwvVn9mPR7BQmD+vOIwu3cv4TS/l2V26gq9apaQD4cNhtnDcigcWbMvzqBrLZhEeuGI0xhgfeXtvsbSWVUq2jR1QIT189lpd/mkxpRTVXPreC/3lnLYeLm1/HSzXPrwAQkWkiskVEtovIg83sc7aIrBaRDSKS6lO+27r372oRSfMpjxWRL0Rkm/UYc+Knc+LOH9WT4opqUrdm+bV/UqyHX184nBU7c/TaAKXayY+GJvDF7EncmjKAd75PZ/K8VN5dla7jcceoxQAQETvwFDAdGA7MEpHhjfaJBp4GLjbGjACuaPQ25xhjxhhjkn3KHgQWG2MGA4ut7YAbPyCOGI+TT/yYDVTrJ6clcfHoXjyycCtvfLu3DWunlKrlcTl4aPowPrr7LPrGeZjz9hqufvEbdrZwRb+q508LYByw3Riz0xhTAbwJzGi0z1XAe8aYvQDGmJZHUb3v8ar1/FXgEr9q3Ma83UA9WLzJv9lA4F1K4pErRpNyUjy/fH/dMYWHUurEDOsZybu3nckfLhnJuv35THtsKY8t2kp5lX9/v8HMnwDoDezz2U63ynydBMSIyFciskpErvN5zQALrfJbfMoTjDEHAazH7sde/bZx8ZheFJVX8erXu/0+xuWw8ew1Yzm1Twz3vvkDX27OaLsKKqUasNmEa87oy+I5KZw3sgePLdrG9MeW8vWOli/sDGb+BIA0Uda4o80BjAUuAM4Dfi0iJ1mvTTDGnIq3C+lOEZl0LBUUkVtEJE1E0rKy/OuXP1HjB8QxZVgCjy7ayt4c/2f3hLrsvPTT0xjSI4KbX03judQd2iepVDvqHhHC32edwqs3jqOqxnDVC98w+63V5BSVB7pqHZI/AZAOJPlsJwKNb4uVDnxmjCk2xmQDS4DRAMaYA9ZjJvA+3i4lgAwR6QlgPTbZbWSMed4Yk2yMSY6Pj/fvrE6QiPD7S0bgsNn4xfvrjulDPCrUyVu3jmf6yJ78+dPN3Pef1X53JSmlWkfKSfEsvH8Sd50ziA/XHGDyvFTe+m6ffiFrxJ8A+A4YLCL9RcQFzAQWNNpnPjBRRBwi4gFOBzaJSJiIRACISBhwLrDeOmYBcL31/HrrPTqMnlGh/HzaEJZtz+a974/tLmAel4MnrzqFB84bwoI1B7jgiaWs2JHTRjVVSjUlxGnn/503hE/umcjg7uH8/N21/OS5lWzTGzrVaTEAjDFVwF3A58Am4C1jzAYRuU1EbrP22QR8BqwFvgVeNMasBxKAZSKyxir/2BjzmfXWfwGmisg2YKq13aFcc3pfTu0Tze8/3sih/GO7GbyIcOc5g3jtxnFUVNcw64WVzH5rNdnaFFWqXQ1OiOA/t4zn/y4fxZaMQs5/YimPfL5FW+aAdKYmUXJysklLS2t5x1a0NaOQS59aTrTHxWs3jWNgfPgxv0dpRTVP/ncbzy/ZidNu45oz+nLzxP50jwhpgxorpZqTXVTOnz7exHs/7KdvnIffzxjJpJPap2s5kERkVaNp+N5yDYCWrUvP54ZXvqW6xvCPG8YxJin6uN5nR1YRT365nfmr9+Ow27j81ERmjUtiVO8oRJoaa1dKtYWvt2fzyw/Wsyu7mItH9+JXFw7r0l/INABO0O7sYq57+VuyCsv55QXDmDWuD3bb8X1o784u5tnUHXywej9llTUM7RHBj8cmMm1kDxJjPK1cc6VUU8oqq3nmqx0889UOQpw2/mf6UGad1gfbcf5dd2QaAK0gq7Ccu9/4npU7cxnRK5LfXTyC5H6xx/1+BWWVLFh9gP98t491+/MBGNk7kinDEjhzYDdGJ0Xhdthbq/pKqSbsyCril++vY+XOXE7tE82fLhvF0B6Rga5Wq9IAaCXGGD5ed5A/fryJg/llTDopnhsm9CNlcPwJfXPYnV3M5xsO8dmGQ6zel4cxEOK0MbZvDGf0j+OMgXGcnKiBoFRbMMbw3vf7+cPHGyksq+Kmif25d/JgPC5HoKvWKjQAWllJRRUvL9vFqyv2kFVYzoBuYVx5WhIXje5F7+jQE3rvvJIKvtmVy8qdOazcmcumgwWA92rjYT0iGNk7yvvTK4qTeoRrKCjVSg4XV/DnTzfxVlo6iTGh/H7GSM4Z2mEWKThuGgBtpKKqhk/XH+TVr3fz/d48AMb1i2X6qB5MHppAn7gT79M/XFzBt7tzSdudy/r9Baw/kE9hWRUADpswOCGCIQnhDOpe/9M3LgynXVf7Vup4fLMzh1+8v44dWcVcMKonv7loOAmRnXeQWAOgHezJKebDNQeYv/oA2zK9KxIO6h7O5KHd+dHQ7oztG4OjFT6UjTHsyy1l3f581h/IZ8OBAnZkFrE/r7RuH4dN6NctjEHx9aHQr1sY/eI8RHtcJ1wHpbq68qpqnk/dyd//ux233cYD04Zw9el9j3vyRyBpALSzPTnFfLk5ky83Z7JyZw6V1YbIEAcTB8dzxsA4zhwYx4BuYa06/bO4vIodWUVsz2z4sye3hGqfm9VEhjjoGxdGnzgPfWM99I3z0DcujL5xHhIiQrrkLAiljtfu7GJ+9cF6lm3PZnRiFH+6bBQjekUFulrHRAMggIrKq1i2LYvFmzJZtj2bg9ZVxd0j3Jw5MI7xA+M4c2A3kmLbZgpoeVU1e3JK2J1dzN7cEvbklLAnt4Q9OcXsP1xKlU84uB02kmK9wZAU6yExJpTEmFB6R3ufR3uces2CCjrGGBasOcDvP9rI4ZJKbjizH/dPPYkwd+cYJNYA6CCMMezJKeHrHTms2JnDih3ZZBd5b2fXOzqU8QPjGD8gjuR+MfSJ9bT5h21VdQ0H8srYk1vMnpwS9ubWB8W+3BKKKxpeLu9x2ekdbYWCTzD0toKiW5hbWxCqy8ovqeQvn23mjW/30isqhN/NGMnU4QmBrlaLNAA6KGMM2zOL+HpHDl/vyGblzlzySysB6BbuJrlvDGP7xjC2Xwwje0XhcrTfwK4xhvzSStIPl5J+uJT9eaWkHy5hv892bV1ruRw2ekeH0js6lJ5RIfSsfYwKoWdUKD2jQ4hwO7QVoTq1tN25/PL99WzJKOTc4Qk8fPEIep3g7L+2pAHQSdTUGLZkFLJqz2FW7TlM2p5c9uV6B3fdDhujE6MZ2y+GsX1iGJ0UTXyEO6D1LSyrZH9eaYNQ2H+4lPS8Ug7mlZJVVE7j/2JhLnuDYOgRFUqvqBB6RIXQyyqPCHEG5oSU8lNldQ0vLt3F44u3Yhdh9rlDuH5831aZ6NHaNAA6scyCMisMvD8b9ufX9dv3igrh5MRoTk6KYnRiNKMSo4jsQB+eldU1ZBSUcSi/jAP5ZRzKL+VAnnf7YH4pB/PLmgyJcLfDCocQekSGkBAZQkKkm+4+z7uFu3Wqqwq4fbkl/Hr+er7aksXI3pH86dJRnJwYHehqNaAB0IWUVVazNj2ftel5rLEe9/jcuWxAtzBOTozyBkNiFCN6RRHq6rgXi1VU1ZBZWMbBfOsnr9R67n3MKCgju6iiwUwmABGIC3PRPcIbCAmRIXSPDKF7hLsuJBIiQ4gLc3XIb2Wq6zDG8Mm6Qzz84QZyisq5bnw/5px7UodpyWoAdHF5JRWs25/P2vR81uzLY216PocKvLON7DZhcPdwRvaOYljPSIb1jGB4z8hOdT1AdY0hp7iczIJyMgrKyLAeMwu9z2sfs5toTdjEO56SYIVDd58WRHyE9Wg978hBqTq+grJKHvl8C6+v3EP3CDcPXzSCaSN7BHzMSwMgCGUWlNW1ENam57PxYAFZhfU3pOkZFcKwnpEM7xlZFwx948I65YUutaqqa8guqrBCooyMwnIyC8q8wVEbFgVl5BRXNHl8mMteHwrWY/1zV4PXQpwaFqppq/fl8dB769h0sIAfDe3O7y4e0WbTvP2hAaAA74qmmw4W+PwUsj2rqK57JdRpZ0iPiLpAGNQ9nMHdI+gW7gr4t5jWVFFVQ25xBdlF5WQVlpNlPWYXlZNdVEFWobfbKbuonLySyibfI8LtoFuEt/XQLcJFfLibuHA3sWEu4sJc3sdwF7FhbqJDnTo9NshUVdfwyte7mbtwKwD3TRnMjWf1D8i4lQaAalZ5VTXbMorY2CgYfKd4xnicDO4ewaCEcAZboTA4IZzuEe4uFQxNqaiqIae4nOzCCrKKyqzH+sDwfSyw1mhqzCYQ4/GGQn0weMMhzmc7LswbIDEep45bdBH780r57fz1LNqUydAeEfzpslGc2iemXeugAaCOiTGGzMJytmUUsS2zkG2ZRWzLKGRrRlGDYIgMcTA4IYLB1npD/buF0b9bGEmxnqCcoVNZXcPh4gpyiivIKaogp7ic3OIKcq2y3KLa597yvNLKI8YsakV7nA1aE7VhEe1xEuNxERPmJNrj8j73OIkM0VZGR2WM4fMNGTy8YAMZhWVcfXofHjhvKFGh7TNIfEIBICLTgMcBO94bvh9xA3cRORt4DHAC2caYFBFJAl4DegA1wPPGmMet/R8GfgZkWW/xC2PMJ0erhwZA4BljyC6qYFtmIdszi9iWUcTWDO9z3351u03oE+uhf7cw+sWF0T8+jAFWOPSI1PWGalVV13C4pLJBKORa4VEfHPXlucUV1DTzJysCUaHecKgNibqw8DiJsh4blrt04LsdFZVXMXfhFl79ejdx4W5+c+FwLjy5Z5u3oo87AETEDmwFpgLpwHfALGPMRp99ooGvgWnGmL0i0t0YkykiPYGexpjvRSQCWAVcYozZaAVAkTHmEX9PQgOgYztcXMGunGJ2ZRWzK7vhT2ll/ZISboetrqXQJ9ZDYqyHPrEekqwlJfT+Bs2rqTEUllVxuKSCwyUV5JVUWs8rybcefctrH0saLenhy+2w1YVCtMdJdKiLqFAnUR4nUaFOIkOtxxCHtzy0vjwYW3mtYV16Pr94fx3r9ucz6aR4/jBjZKssHd+c5gLAn5WMxgHbjTE7rTd6E5gBbPTZ5yrgPWPMXgBjTKb1eBA4aD0vFJFNQO9Gx6ouIibMRUyY64j+TWMMGQXl7MwuYld2MbutUNiSUcjiTZlUVNfU7SsCPSJDSIrxLkaXFBvqDYdYD0kxHrpHBPdaQzabeD+YPU76Eeb3ceVV1eSXVPoERH1Y5PuESF5JBTuyvN18+aWVlFfVHPV9w1z2+oDwCYco39DwOI8IjsgQZ1DPohqVGMUHd07gtRW7eeTzLUx9NJV7Jg/mZxMHtOtyL/60AH6M95v9zdb2tcDpxpi7fPZ5DG/XzwggAnjcGPNao/fpBywBRhpjCqwWwE+BAiANmGOMOdzE778FuAWgT58+Y/fs2XM856k6qJoa71jDvsMl7M0p8T7mlpCeW8q+wyUcKihr0EfucthIjPGGQu/oUHpFh9Ir2rvOUO/oUBIiQ9r1D6irK6uspqCskgIrEOp+SiopKKtqWFbq3a9238YLCTbmdtjqAiHc7SAixEFkiJOIEIf1U18eEeINkwjr9XBrn67QWjyYX8rvFmzksw2HOCkhnD9eOorTTuBe4005kS6gK4DzGgXAOGPM3T77PAkkA5OBUGAFcIExZqv1ejiQCvzRGPOeVZYAZAMG+D3erqIbj1YX7QIKPuVV1ew/XOpdnfRwKfusVUr35pZwIK+Uw42maIp10VevunWFvAFRu8ZQr+hQ4sODuxXRXiqraxoEh29gFPgESWF5JYVlVRSUVVFU5n1eWFbVoNuwOS6HrWEw+ASGb3B4yxuGS2SIgzC3A4/L3iFmsi3amMFvF2xgf14pM09L4sHpQ1vtYs0T6QJKB5J8thOBA03sk22MKQaKRWQJMBrYKiJO4F3gX7Uf/gDGmAyfyr0AfOTvyajg4XbYGRAfzoD48CZfL6mospaPKONAXikH8ku9z/NL2ZpRSOrWrCP6vx02abBUhHcJCXfDJSUi3ESF6r0PToTTbiPOujbieFRW11BkhUFBWSVF5VVWOFQ2eCywnte+np1dXBciReVNT8v1JQJhLgdhbjthbm9YeLcdhPuWuRuW+e4bEVL7uv24WyVThicwfmAcjy/exkvLdvHFxgx+deEwLhnTu83+H/rTAnDgHQSeDOzHOwh8lTFmg88+w4AngfMAF/AtMBPYALwK5Bpj7mv0vj2tMQJE5H683Uozj1YXbQGoY1W7pPWBPO/aQt6Q8K43lFlYu5xEed09ln25HDZvIET4BoQ3HOIj3MSFey/+iglz6WBoB1VdYygqr7LC4cjgKC73/hSV1z6vrnteVF5FcUV9WUUL4yG1nHbxhoGrNjgahkhtmcflIMxlx2Pt63Hb8Ti9++7JKeH/PtvM3twSJgyK4w+XjKJ/N//HfBo70Wmg5+Od4mkHXjbG/FFEbgMwxjxr7fMAcAPe6Z4vGmMeE5GzgKXAOqscrOmeIvI6MAZvF9Bu4NbaQGiOBoBqKyUVVWQWlNeFQkZBGVmF9esOZRZ6l5MobOYbZbTHSbdw7zx97/IRLuvbr892mJtuEW7COkiXgzo2ldU1PmFR7RMaPgFS0ShAmtu3ovqIxQ2PxuWwsXh2ynEvJ6EXginVCkoqqsgoKCenqH7ZiBxryYjaq4Wzi8vJKao44mY5tdwOW10oeK/6dRHtcRHb6MKuaOvK4WhPcM+Y6YqMMVRU11BSXk1xRRUlFdUUlzd6rKiqe90uwk0T++NxHd8tKE9kDEApZfG4HPTv5vCrOe673lB2UblPUFTUrzlUVM7WjKIW5+qHOu1HhELjoPDOqHFY0y+9s2s0ODomEcHt8I4XxIQFblVeDQCl2ojLYfPe0CYqxK/9y6uq6y/sKvbOyc+tvZiruH6efm5JBfvzSr1z+I+ylERtHSJDnESFOnwu6HIeERYNL/jyvhYR4uzUK8OqlmkAKNVBuB12EiLtJET6FxjgHeQsKK2sC4PaqZYFdVMva+fme2fS5BZXsDu7uG7flvqhI2oHLn2mWIa5Gm2765/7DnT6brsdNh336IA0AJTqxOw2qbsC+1gZYyiuqK4Pi2YCpHbgsqi8mqKySjIKyigqq6qbXePPWKbTLs2GQ90MGJd3ZozHZSfM5SDUZSfMbSfUWTtrpv51j8uhF/y1Ag0ApYKUiPdDOdztoFd06HG9hzGG0spq73z92lku1vOiMu80ytr5+L6hUVRWRXZRBbtzSigur6LUGvQ8hokxOGziDYnaUHDb8TiPDJNQl51Qp50Qp/cx1GnH7bTVl7nshDjshLpsuB3WttNOiMPW5Zfk1gBQSh03EbE+aB10P8H3MsZQXlVDSUU1JRW1oeB9XlJeTUllNaXWvPzSyvrZMiXWLJraEDlcXEH64frtkopqv+fwN+a0izcMnLUhYrMCpOF2SIMfmzXAa8Pt+9xhw20Fi9vZsKzuucOO0y7t1l2mAaCU6hBE6j9sY1t5ZkxNjaGsqpqyyhpKK6spq/QGRHlVNaUVNd5tq7z+eU2DsrLKGkorqimr8h6bV1pJWX79du2+ldUnNrVehLow8A2RP106inH9W3eNIA0ApVSXZ7PVtlTa/ndV1xgqqmoor6qmvKqG8kqf51XV1nZzr9dQXunz3Gf/cHfrf1xrACilVCuyW2MTneFGO117hEMppVSzNACUUipIaQAopVSQ0gBQSqkgpQGglFJBSgNAKaWClAaAUkoFKQ0ApZQKUp3qjmAikgXsOc7DuwHZrVidziIYzzsYzxmC87yD8Zzh2M+7rzEmvnFhpwqAEyEiaU3dEq2rC8bzDsZzhuA872A8Z2i989YuIKWUClIaAEopFaSCKQCeD3QFAiQYzzsYzxmC87yD8Zyhlc47aMYAlFJKNRRMLQCllFI+NACUUipIBUUAiMg0EdkiIttF5MFA16ctiEiSiPxXRDaJyAYRudcqjxWRL0Rkm/UYE+i6tjYRsYvIDyLykbUdDOccLSLviMhm6998fFc/bxG53/q/vV5E3hCRkK54ziLysohkish6n7Jmz1NEHrI+27aIyHnH8ru6fACIiB14CpgODAdmicjwwNaqTVQBc4wxw4AzgDut83wQWGyMGQwstra7mnuBTT7bwXDOjwOfGWOGAqPxnn+XPW8R6Q3cAyQbY0YCdmAmXfOcXwGmNSpr8jytv/GZwAjrmKetzzy/dPkAAMYB240xO40xFcCbwIwA16nVGWMOGmO+t54X4v1A6I33XF+1dnsVuCQgFWwjIpIIXAC86FPc1c85EpgEvARgjKkwxuTRxc8b7y1sQ0XEAXiAA3TBczbGLAFyGxU3d54zgDeNMeXGmF3AdryfeX4JhgDoDezz2U63yrosEekHnAJ8AyQYYw6CNySA7gGsWlt4DPg5UONT1tXPeQCQBfzD6vp6UUTC6MLnbYzZDzwC7AUOAvnGmIV04XNupLnzPKHPt2AIAGmirMvOfRWRcOBd4D5jTEGg69OWRORCINMYsyrQdWlnDuBU4BljzClAMV2j66NZVp/3DKA/0AsIE5FrAlurDuGEPt+CIQDSgSSf7US8TccuR0SceD/8/2WMec8qzhCRntbrPYHMQNWvDUwALhaR3Xi79n4kIv+ka58zeP9PpxtjvrG238EbCF35vKcAu4wxWcaYSuA94Ey69jn7au48T+jzLRgC4DtgsIj0FxEX3gGTBQGuU6sTEcHbJ7zJGDPP56UFwPXW8+uB+e1dt7ZijHnIGJNojOmH99/1S2PMNXThcwYwxhwC9onIEKtoMrCRrn3ee4EzRMRj/V+fjHecqyufs6/mznMBMFNE3CLSHxgMfOv3uxpjuvwPcD6wFdgB/DLQ9WmjczwLb9NvLbDa+jkfiMM7a2Cb9Rgb6Lq20fmfDXxkPe/y5wyMAdKsf+8PgJiuft7A74DNwHrgdcDdFc8ZeAPvOEcl3m/4Nx3tPIFfWp9tW4Dpx/K7dCkIpZQKUsHQBaSUUqoJGgBKKRWkNACUUipIaQAopVSQ0gBQSqkgpQGglFJBSgNAKaWC1P8HIIETosL/bBsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as pyplot\n",
    "\n",
    "pyplot.plot(iteration_vec, loss_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column:  [  6.    148.     72.     35.      0.     33.6     0.627  50.   ]\n",
      "Weights:  [0.493796   0.289154   0.369878   0.461918   0.370004   0.4426352\n",
      " 0.49916111 0.440312  ]\n",
      "Dot product:  125.75703073597\n",
      "yhat:  1.177059699480968e-41\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'yhat_random_bias' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-83-3ae7139e28a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0myhat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mone_column\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m220\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"yhat: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myhat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"yhat random bias: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myhat_random_bias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myhat_random_bias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Loss: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'yhat_random_bias' is not defined"
     ]
    }
   ],
   "source": [
    "one_column = training_data[0:8, 0]\n",
    "y = training_data[8, 0]\n",
    "\n",
    "print(\"column: \", one_column)\n",
    "print(\"Weights: \", weights)\n",
    "print(\"Dot product: \", np.dot(one_column, weights))\n",
    "yhat = sigmoid(one_column, weights, -220)\n",
    "print(\"yhat: \", yhat)\n",
    "print(\"yhat random bias: \", yhat_random_bias)\n",
    "loss = loss(y, yhat_random_bias)\n",
    "print(\"Loss: \", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(one_column, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = sigmoid(np.array([0.5, 0.5]), np.array([1, 1]), 0)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sig(X, weight):\n",
    "    z = np.dot(X, weight) + 0\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "sig(0.5, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "\n",
    "\n",
    "z = np.array([ 0.2, 0.4, 0.1])\n",
    "g = expit(z)\n",
    "\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
