{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1\n",
    "\n",
    "## Basic idea for gradient descent\n",
    "### Not logistic regression classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron(input, weights):\n",
    "    out = 0\n",
    "    for i in range(len(input)):\n",
    "        out += (input[i] * weights[i])\n",
    "    return out\n",
    "\n",
    "# \n",
    "def ele_mul(scalar, vector):\n",
    "    out = [0,0,0]\n",
    "    for i in range(len(out)):\n",
    "        out[i] = vector[i] * scalar\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Pred: 0.8600000000000001\n",
      "Error: 0.01959999999999997\n",
      "Delta: -0.1399999999999999\n",
      "Weights: [0.1, 0.2, -0.1]\n",
      "Weight_Deltas: [-1.189999999999999, -0.09099999999999994, -0.16799999999999987]\n",
      "\n",
      "Iteration: 2\n",
      "Pred: 0.9637574999999999\n",
      "Error: 0.0013135188062500048\n",
      "Delta: -0.036242500000000066\n",
      "Weights: [0.1119, 0.20091, -0.09832]\n",
      "Weight_Deltas: [-0.30806125000000056, -0.023557625000000044, -0.04349100000000008]\n",
      "\n",
      "Iteration: 3\n",
      "Pred: 0.9906177228125002\n",
      "Error: 8.802712522307997e-05\n",
      "Delta: -0.009382277187499843\n",
      "Weights: [0.11498061250000001, 0.20114557625, -0.09788509000000001]\n",
      "Weight_Deltas: [-0.07974935609374867, -0.006098480171874899, -0.011258732624999811]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature1 = [8.5, 9.5, 9.9, 9.0]\n",
    "feature2 = [0.65, 0.8, 0.8, 0.9]\n",
    "feature3 = [1.2, 1.3, 0.5, 1.0]\n",
    "\n",
    "win_or_lose_binary = [1, 1, 0, 1]\n",
    "\n",
    "# y\n",
    "true = win_or_lose_binary[0]\n",
    "\n",
    "# Alpha is the learning rate\n",
    "alpha = 0.01\n",
    "weights = [0.1, 0.2, -.1]\n",
    "input = [feature1[0], feature2[0], feature3[0]]\n",
    "\n",
    "for iter in range(3):\n",
    "    # yhat\n",
    "    pred = neuron(input, weights)\n",
    "    \n",
    "    # Mean squared loss function\n",
    "    error = (pred - true) ** 2\n",
    "    \n",
    "    # loss calculation\n",
    "    delta = pred - true\n",
    "    \n",
    "    \n",
    "    weight_deltas = ele_mul(delta, input)\n",
    "    \n",
    "    print(\"Iteration: \" + str(iter+1))\n",
    "    print(\"Pred: \"+ str(pred))\n",
    "    print(\"Error: \" + str(error))\n",
    "    print(\"Delta: \" + str(delta))\n",
    "    print(\"Weights: \" + str(weights))\n",
    "    print(\"Weight_Deltas: \" + str(weight_deltas))\n",
    "    print()\n",
    "    \n",
    "    for i in range(len(weights)):\n",
    "        # change the weights by the alpha * how much each weight needs to change\n",
    "        weights[i] -= alpha*weight_deltas[i]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>10</td>\n",
       "      <td>101</td>\n",
       "      <td>76</td>\n",
       "      <td>48</td>\n",
       "      <td>180</td>\n",
       "      <td>32.9</td>\n",
       "      <td>0.171</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>70</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.340</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>5</td>\n",
       "      <td>121</td>\n",
       "      <td>72</td>\n",
       "      <td>23</td>\n",
       "      <td>112</td>\n",
       "      <td>26.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1</td>\n",
       "      <td>126</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.349</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>70</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>30.4</td>\n",
       "      <td>0.315</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0              6      148             72             35        0  33.6   \n",
       "1              1       85             66             29        0  26.6   \n",
       "2              8      183             64              0        0  23.3   \n",
       "3              1       89             66             23       94  28.1   \n",
       "4              0      137             40             35      168  43.1   \n",
       "..           ...      ...            ...            ...      ...   ...   \n",
       "763           10      101             76             48      180  32.9   \n",
       "764            2      122             70             27        0  36.8   \n",
       "765            5      121             72             23      112  26.2   \n",
       "766            1      126             60              0        0  30.1   \n",
       "767            1       93             70             31        0  30.4   \n",
       "\n",
       "     DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                       0.627   50        1  \n",
       "1                       0.351   31        0  \n",
       "2                       0.672   32        1  \n",
       "3                       0.167   21        0  \n",
       "4                       2.288   33        1  \n",
       "..                        ...  ...      ...  \n",
       "763                     0.171   63        0  \n",
       "764                     0.340   27        0  \n",
       "765                     0.245   30        0  \n",
       "766                     0.349   47        1  \n",
       "767                     0.315   23        0  \n",
       "\n",
       "[768 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('diabetes.csv', sep=',', header='infer')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Outcome</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.298000</td>\n",
       "      <td>109.980000</td>\n",
       "      <td>68.184000</td>\n",
       "      <td>19.664000</td>\n",
       "      <td>68.792000</td>\n",
       "      <td>30.304200</td>\n",
       "      <td>0.429734</td>\n",
       "      <td>31.190000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.865672</td>\n",
       "      <td>141.257463</td>\n",
       "      <td>70.824627</td>\n",
       "      <td>22.164179</td>\n",
       "      <td>100.335821</td>\n",
       "      <td>35.142537</td>\n",
       "      <td>0.550500</td>\n",
       "      <td>37.067164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "Outcome                                                                      \n",
       "0           3.298000  109.980000      68.184000      19.664000   68.792000   \n",
       "1           4.865672  141.257463      70.824627      22.164179  100.335821   \n",
       "\n",
       "               BMI  DiabetesPedigreeFunction        Age  \n",
       "Outcome                                                  \n",
       "0        30.304200                  0.429734  31.190000  \n",
       "1        35.142537                  0.550500  37.067164  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('Outcome').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pregnancies                   int64\n",
       "Glucose                       int64\n",
       "BloodPressure                 int64\n",
       "SkinThickness                 int64\n",
       "Insulin                       int64\n",
       "BMI                         float64\n",
       "DiabetesPedigreeFunction    float64\n",
       "Age                           int64\n",
       "Outcome                       int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 768)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = df.to_numpy()\n",
    "\n",
    "data = np.transpose(raw_data)\n",
    "\n",
    "data[8] # training labels (Y)\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw:  [  6.    148.     72.     35.      0.     33.6     0.627  50.      1.   ]\n",
      "Training data size:  (8, 768)\n",
      "Testing data size:  (8, 268)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.804065  ,  2.26906711,  0.62429218, -0.17645351, -0.93391565,\n",
       "       -0.206752  , -0.92034626,  0.14817312])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: slice upper bound is exclusive\n",
    "raw_training_data = data[:, :]\n",
    "raw_testing_data = data[:, 500:]\n",
    "\n",
    "print(\"Raw: \", raw_training_data[:, 0])\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "training_data = preprocessing.scale(raw_training_data[0:8, :], axis=0) # ~68%\n",
    "testing_data = preprocessing.scale(raw_testing_data[0:8, :], axis=0) # ~68%\n",
    "\n",
    "# min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# training_data = min_max_scaler.fit_transform(raw_training_data[0:8, :]) # ~61%\n",
    "# testing_data = min_max_scaler.fit_transform(raw_testing_data[0:8, :]) # ~61%\n",
    "\n",
    "print(\"Training data size: \", training_data.shape)\n",
    "print(\"Testing data size: \", testing_data.shape)\n",
    "\n",
    "training_data[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration_vec = []\n",
    "loss_vec = []\n",
    "def store_loss(loss, iteration):\n",
    "    iteration_vec.append(iteration)\n",
    "    loss_vec.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid activation function\n",
    "def sigmoid(input_vector, weight_vector, bias):\n",
    "    z = np.dot(input_vector, weight_vector) + bias\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "\n",
    "# loss function\n",
    "def wierd_broken_loss(y, yhat):\n",
    "    return (-y * np.log(yhat) - (1 - y) * np.log(1 - yhat))\n",
    "\n",
    "def loss(y, yhat):\n",
    "    #if yhat == 1.0:\n",
    "    #    return 1.0\n",
    "    #if yhat == 0:\n",
    "    #    return 0.0\n",
    "    return -(y * np.log(yhat) + (1-y) * np.log(1-yhat))\n",
    "\n",
    "# From lab sheet tutorial\n",
    "def loss_other(y, h):\n",
    "    return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "\n",
    "# Derivative of sigmoid\n",
    "def dz_calc(pred, actual):\n",
    "    return pred - actual\n",
    "    \n",
    "def update_dweights(features, dz, derivative_weights):\n",
    "    for i in range(len(derivative_weights)):\n",
    "        derivative_weights[i] += features[i] * dz\n",
    "    return derivative_weights\n",
    "\n",
    "def update_bias(db, dz):\n",
    "    db += dz\n",
    "    return db\n",
    "\n",
    "# Init random weights and bias\n",
    "weights = np.random.rand(training_data.shape[0])\n",
    "initial_weights = weights\n",
    "bias = np.random.rand(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent algorithm from slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight derivatives:  [-1.17674521e-02  1.96462617e-02  4.67308009e-03 -1.17414589e-02\n",
      "  1.94553323e-02 -4.65557435e-05 -1.21027751e-02 -8.11643221e-03]\n",
      "Weights on iteration  0 :  [ 6.11081705  0.42619545 -0.57648354  0.23483984  0.39950627  1.41207631\n",
      " -3.1567586  -0.85019279]\n",
      "Loss:  0.6027378053419178\n",
      "Weight derivatives:  [ 0.00099273 -0.00030193 -0.00041125 -0.00452793  0.00099918  0.00461649\n",
      "  0.00147192 -0.00283921]\n",
      "Weights on iteration  10 :  [ 6.1082661   0.42938754 -0.57056179  0.26418606  0.37322057  1.38707742\n",
      " -3.1624541  -0.82912182]\n",
      "Loss:  0.6016515359274685\n",
      "Weight derivatives:  [ 0.00094523 -0.0001738  -0.00014959 -0.00416405  0.00025943  0.00456973\n",
      "  0.00138053 -0.00266747]\n",
      "Weights on iteration  20 :  [ 6.10238192  0.43054047 -0.56918191  0.29013074  0.37058439  1.35940962\n",
      " -3.17105144 -0.8128138 ]\n",
      "Loss:  0.601340850502714\n",
      "Weight derivatives:  [ 8.45841883e-04 -2.08026119e-04 -7.14912900e-05 -3.83820618e-03\n",
      "  1.96700760e-04  4.43330771e-03  1.25055658e-03 -2.60868335e-03]\n",
      "Weights on iteration  30 :  [ 6.09704147  0.43169163 -0.56856631  0.31402309  0.36928318  1.33243593\n",
      " -3.17890137 -0.79700763]\n",
      "Loss:  0.6010615434308361\n",
      "Weight derivatives:  [ 7.57277384e-04 -2.37490667e-04 -2.49910932e-05 -3.54388359e-03\n",
      "  1.71211612e-04  4.29413642e-03  1.13305174e-03 -2.54931181e-03]\n",
      "Weights on iteration  40 :  [ 6.09226492  0.43304232 -0.56830141  0.33606642  0.36819077  1.30629608\n",
      " -3.18601018 -0.78154894]\n",
      "Loss:  0.6008089163275412\n",
      "Weight derivatives:  [ 6.80544207e-04 -2.57069193e-04  3.66854118e-06 -3.27745291e-03\n",
      "  1.49952755e-04  4.15705426e-03  1.02834480e-03 -2.48504246e-03]\n",
      "Weights on iteration  50 :  [ 6.0879799   0.43453621 -0.56825338  0.3564375   0.36723494  1.28098485\n",
      " -3.19245708 -0.76646295]\n",
      "Loss:  0.6005797523495242\n",
      "Weight derivatives:  [ 6.13897816e-04 -2.69082601e-04  1.97067225e-05 -3.03584554e-03\n",
      "  1.31017735e-04  4.02260398e-03  9.34602998e-04 -2.41690111e-03]\n",
      "Weights on iteration  60 :  [ 6.08412115  0.43612146 -0.56833357  0.37529332  0.36639877  1.25648759\n",
      " -3.1983128  -0.75177593]\n",
      "Loss:  0.6003713696527415\n",
      "Weight derivatives:  [ 5.55785593e-04 -2.75588765e-04  2.67790183e-05 -2.81637257e-03\n",
      "  1.14104631e-04  3.89103320e-03  8.50233926e-04 -2.34597504e-03]\n",
      "Weights on iteration  70 :  [ 6.08063342  0.43775972 -0.56847884  0.39277379  0.36566942  1.23278762\n",
      " -3.20363771 -0.73750744]\n",
      "Loss:  0.6001814982812992\n",
      "Weight derivatives:  [ 5.04930493e-04 -2.78140527e-04  2.75991075e-05 -2.61666962e-03\n",
      "  9.90094282e-05  3.76251450e-03  7.73935762e-04 -2.27317914e-03]\n",
      "Weights on iteration  80 :  [ 6.07746984  0.43942332 -0.56864476  0.40900376  0.36503546  1.20986708\n",
      " -3.20848361 -0.72367109]\n",
      "Loss:  0.6000081939900351\n",
      "Weight derivatives:  [ 4.60281950e-04 -2.77891466e-04  2.41771216e-05 -2.43465419e-03\n",
      "  8.55490434e-05  3.63716883e-03  7.04642161e-04 -2.19927346e-03]\n",
      "Weights on iteration  90 :  [ 6.07459044  0.44109249 -0.56880075  0.42409484  0.36448659  1.18770723\n",
      " -3.21289532 -0.71027552]\n",
      "Loss:  0.5998497752138888\n",
      "Weight derivatives:  [ 4.20971020e-04 -2.75692533e-04  1.79913361e-05 -2.26848884e-03\n",
      "  7.35568357e-05  3.51507650e-03  6.41473899e-04 -2.12488823e-03]\n",
      "Weights on iteration  100 :  [ 6.07196093  0.44275338 -0.56892647  0.438147    0.36401356  1.16628874\n",
      " -3.21691188 -0.69732525]\n",
      "Loss:  0.599704775767392\n",
      "Weight derivatives:  [ 3.86274587e-04 -2.72166979e-04  1.01193239e-05 -2.11654948e-03\n",
      "  6.28815664e-05  3.39628533e-03  5.83700608e-04 -2.05054496e-03]\n",
      "Weights on iteration  110 :  [ 6.06955173  0.44439643 -0.56900906  0.45124986  0.36360806  1.14559194\n",
      " -3.22056756 -0.68482139]\n",
      "Loss:  0.5995719085203673\n",
      "Weight derivatives:  [ 3.55587068e-04 -2.67767572e-04  1.33806932e-06 -1.97739804e-03\n",
      "  5.33863982e-05  3.28081716e-03  5.30710747e-04 -1.97667384e-03]\n",
      "Weights on iteration  120 :  [ 6.0673372   0.44601525 -0.5690411   0.46348396  0.36326266  1.12559692\n",
      " -3.22389267 -0.67276223]\n",
      "Loss:  0.5994500369379404\n",
      "Weight derivatives:  [ 3.28398068e-04 -2.62820056e-04 -7.79991067e-06 -1.84975896e-03\n",
      "  4.49478362e-05  3.16867306e-03  4.81988073e-04 -1.90362811e-03]\n",
      "Weights on iteration  130 :  [ 6.06529502  0.44760574 -0.56901904  0.47492173  0.36297068  1.10628373\n",
      " -3.22691415 -0.66114372]\n",
      "Loss:  0.5993381523990652\n",
      "Weight derivatives:  [ 3.04274700e-04 -2.57556021e-04 -1.69089036e-05 -1.73249900e-03\n",
      "  3.74546539e-05  3.05983738e-03  4.37093181e-04 -1.83169599e-03]\n",
      "Weights on iteration  140 :  [ 6.06340566  0.4491654  -0.56894209  0.48562845  0.36272617  1.08763248\n",
      " -3.22965613 -0.64995994]\n",
      "Loss:  0.5992353558589456\n",
      "Weight derivatives:  [ 2.82847568e-04 -2.52137656e-04 -2.57264645e-05 -1.62460975e-03\n",
      "  3.08068405e-05  2.95428108e-03  3.95649029e-04 -1.76111065e-03]\n",
      "Weights on iteration  150 :  [ 6.06165197  0.4506929  -0.56881135  0.495663    0.36252377  1.06962341\n",
      " -3.2321403  -0.63920341]\n",
      "Loss:  0.5991408428424029\n",
      "Weight derivatives:  [ 2.63799620e-04 -2.46676336e-04 -3.40806503e-05 -1.52519260e-03\n",
      "  2.49145919e-05  2.85196441e-03  3.57329525e-04 -1.69205856e-03]\n",
      "Weights on iteration  160 :  [ 6.06001885  0.4521877  -0.56862916  0.5050786   0.36235873  1.05223696\n",
      " -3.23438627 -0.62886542]\n",
      "Loss:  0.5990538910346951\n",
      "Weight derivatives:  [ 2.46857236e-04 -2.41246514e-04 -4.18654385e-05 -1.43344568e-03\n",
      "  1.96973582e-05  2.75283900e-03  3.21850524e-04 -1.62468649e-03]\n",
      "Weights on iteration  170 :  [ 6.05849294  0.45364981 -0.56839868  0.51392337  0.36222677  1.03545385\n",
      " -3.23641182 -0.61893626]\n",
      "Loss:  0.5989738499265719\n",
      "Weight derivatives:  [ 2.31783074e-04 -2.35896062e-04 -4.90223478e-05 -1.34865248e-03\n",
      "  1.50829557e-05  2.65684970e-03  2.88962688e-04 -1.55910752e-03]\n",
      "Weights on iteration  180 :  [ 6.05706241  0.45507959 -0.56812355  0.52224095  0.3621241   1.01925512\n",
      " -3.23823317 -0.60940546]\n",
      "Loss:  0.598900132103063\n",
      "Weight derivatives:  [ 2.18370298e-04 -2.30653942e-04 -5.55267477e-05 -1.27017206e-03\n",
      "  1.10067461e-05  2.56393595e-03  2.58445785e-04 -1.49540603e-03]\n",
      "Weights on iteration  190 :  [ 6.05571675  0.45647761 -0.56780763  0.53007091  0.3620473   1.00362214\n",
      " -3.23986512 -0.60026198]\n",
      "Loss:  0.5988322058607285\n",
      "Weight derivatives:  [ 2.06437884e-04 -2.25535866e-04 -6.13777018e-05 -1.19743030e-03\n",
      "  7.41088365e-06  2.47403302e-03  2.30104127e-04 -1.43364205e-03]\n",
      "Weights on iteration  200 :  [ 6.0544466   0.45784458 -0.56745484  0.53744918  0.36199335  0.98853668\n",
      " -3.24132123 -0.59149433]\n",
      "Loss:  0.5987695889078287\n",
      "Weight derivatives:  [ 1.95826789e-04 -2.20548437e-04 -6.65904723e-05 -1.12991243e-03\n",
      "  4.24362656e-06  2.38707297e-03  2.03762887e-04 -1.37385494e-03]\n",
      "Weights on iteration  210 :  [ 6.05324361  0.45918127 -0.56706906  0.54440849  0.36195954  0.97398089\n",
      " -3.24261398 -0.58309076]\n",
      "Loss:  0.5987118429540773\n",
      "Weight derivatives:  [ 1.86396791e-04 -2.15692171e-04 -7.11910196e-05 -1.06715636e-03\n",
      "  1.45871235e-06  2.30298549e-03  1.79265107e-04 -1.31606655e-03]\n",
      "Weights on iteration  220 :  [ 6.05210032  0.46048847 -0.56665404  0.55097862  0.36194345  0.95993735\n",
      " -3.24375484 -0.57503933]\n",
      "Loss:  0.5986585690362973\n",
      "Weight derivatives:  [ 1.78023877e-04 -2.10963680e-04 -7.52119964e-05 -1.00874686e-03\n",
      " -9.85209021e-07  2.22169857e-03  1.56469252e-04 -1.26028395e-03]\n",
      "Weights on iteration  230 :  [ 6.05101006  0.46176696 -0.56621335  0.55718675  0.36194292  0.94638905\n",
      " -3.2447544  -0.567328  ]\n",
      "Loss:  0.5986094034569204\n",
      "Weight derivatives:  [ 1.70598064e-04 -2.06357219e-04 -7.86898581e-05 -9.54310386e-04\n",
      " -3.12508689e-06  2.14313910e-03  1.35247188e-04 -1.20650180e-03]\n",
      "Weights on iteration  240 :  [ 6.04996687  0.46301748 -0.56575034  0.56305771  0.36195603  0.93331944\n",
      " -3.24562243 -0.55994477]\n",
      "Loss:  0.5985640142360961\n",
      "Weight derivatives:  [ 1.64021570e-04 -2.01865786e-04 -8.16628017e-05 -9.03510566e-04\n",
      " -4.99393137e-06  2.06723335e-03  1.15482510e-04 -1.15470434e-03]\n",
      "Weights on iteration  250 :  [ 6.04896538  0.46424075 -0.56526815  0.56861421  0.36198108  0.92071239\n",
      " -3.24636799 -0.55287766]\n",
      "Loss:  0.5985220979969211\n",
      "Weight derivatives:  [ 1.58207262e-04 -1.97481876e-04 -8.41693209e-05 -8.56044169e-04\n",
      " -6.62122855e-06  1.99390735e-03  9.70691369e-05 -1.10486716e-03]\n",
      "Weights on iteration  260 :  [ 6.04800079  0.46543742 -0.56476968  0.57387705  0.36201652  0.90855222\n",
      " -3.24699948 -0.54611486]\n",
      "Loss:  0.5984833772181549\n",
      "Weight derivatives:  [ 1.53077349e-04 -1.93197989e-04 -8.62472143e-05 -8.11637555e-04\n",
      " -8.03331348e-06  1.92308731e-03  7.99101278e-05 -1.05695871e-03]\n",
      "Weights on iteration  270 :  [ 6.0470688   0.46660813 -0.5642576   0.57886533  0.36206101  0.8968237\n",
      " -3.24752467 -0.53964471]\n",
      "Loss:  0.5984475978005569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight derivatives:  [ 1.48562254e-04 -1.89006958e-04 -8.79329265e-05 -7.70043517e-04\n",
      " -9.25370484e-06  1.85469980e-03  6.39166750e-05 -1.01094162e-03]\n",
      "Weights on iteration  280 :  [ 6.04616552  0.46775344 -0.56373437  0.58359656  0.36211333  0.88551205\n",
      " -3.2479508  -0.53345574]\n",
      "Loss:  0.5984145269024701\n",
      "Weight derivatives:  [ 1.44599647e-04 -1.84902148e-04 -8.92611314e-05 -7.31038486e-04\n",
      " -1.03034050e-05  1.78867210e-03  4.90072421e-05 -9.66773822e-04]\n",
      "Weights on iteration  290 :  [ 6.04528749  0.4688739  -0.56320222  0.58808687  0.36217239  0.87460289\n",
      " -3.24828458 -0.52753674]\n",
      "Loss:  0.5983839510078544\n",
      "Weight derivatives:  [ 1.41133614e-04 -1.80877567e-04 -9.02644909e-05 -6.94420041e-04\n",
      " -1.12011684e-05  1.72493235e-03  3.51068207e-05 -9.24409513e-04]\n",
      "Weights on iteration  300 :  [ 6.04443156  0.46996999 -0.56266319  0.59235113  0.36223725  0.86408231\n",
      " -3.24853227 -0.52187678]\n",
      "Loss:  0.5983556741960814\n",
      "Weight derivatives:  [ 1.38113930e-04 -1.76927911e-04 -9.09735373e-05 -6.60004704e-04\n",
      " -1.19637425e-05  1.66340970e-03  2.21462857e-05 -8.83800021e-04]\n",
      "Weights on iteration  310 :  [ 6.04359493  0.47104219 -0.56211913  0.59640303  0.36230703  0.85393682\n",
      " -3.24869969 -0.51646519]\n",
      "Loss:  0.5983295165877839\n",
      "Weight derivatives:  [ 1.35495430e-04 -1.73048566e-04 -9.14166442e-05 -6.27625969e-04\n",
      " -1.26060818e-05  1.60403452e-03  1.00618313e-05 -8.44894516e-04]\n",
      "Weights on iteration  320 :  [ 6.04277508  0.47209092 -0.5615717   0.60025524  0.36238099  0.84415335\n",
      " -3.24879227 -0.51129161]\n",
      "Loss:  0.5983053129451036\n",
      "Weight derivatives:  [ 1.33237454e-04 -1.69235580e-04 -9.16200578e-05 -5.97132553e-04\n",
      " -1.31415411e-05  1.54673843e-03 -1.20552281e-06 -8.07640633e-04]\n",
      "Weights on iteration  330 :  [ 6.04196973  0.47311659 -0.56102241  0.60391947  0.36245845  0.83471923\n",
      " -3.24881507 -0.50634599]\n",
      "Loss:  0.5982829114079337\n",
      "Weight derivatives:  [ 1.31303368e-04 -1.65485621e-04 -9.16079670e-05 -5.68386836e-04\n",
      " -1.35820460e-05  1.49145448e-03 -1.17103688e-05 -7.71985008e-04]\n",
      "Weights on iteration  340 :  [ 6.04117684  0.4741196  -0.56047263  0.60740657  0.36253879  0.82562222\n",
      " -3.24877281 -0.50161859]\n",
      "Loss:  0.5982621723505045\n",
      "Weight derivatives:  [ 1.29660130e-04 -1.61795922e-04 -9.14026008e-05 -5.41263469e-04\n",
      " -1.39382473e-05  1.43811714e-03 -2.15032975e-05 -7.37873730e-04]\n",
      "Weights on iteration  350 :  [ 6.04039457  0.47510031 -0.55992357  0.61072661  0.3626215   0.81685045\n",
      " -3.24866989 -0.49709999]\n",
      "Loss:  0.5982429673448629\n",
      "Weight derivatives:  [ 1.28277921e-04 -1.58164223e-04 -9.10243393e-05 -5.15648137e-04\n",
      " -1.42196568e-05  1.38666241e-03 -3.06312430e-05 -7.05252728e-04]\n",
      "Weights on iteration  360 :  [ 6.0396213   0.47605907 -0.55937632  0.61388894  0.36270609  0.80839246\n",
      " -3.24851043 -0.49278113]\n",
      "Loss:  0.5982251782196691\n",
      "Weight derivatives:  [ 1.27129806e-04 -1.54588709e-04 -9.04918330e-05 -4.91436450e-04\n",
      " -1.44347703e-05  1.33702785e-03 -3.91377912e-05 -6.74068098e-04]\n",
      "Weights on iteration  370 :  [ 6.03885553  0.47699623 -0.55883186  0.61690226  0.36279215  0.80023717\n",
      " -3.24829827 -0.48865322]\n",
      "Loss:  0.5982086962042423\n",
      "Weight derivatives:  [ 1.26191445e-04 -1.51067953e-04 -8.98221237e-05 -4.68532954e-04\n",
      " -1.45911763e-05  1.28915260e-03 -4.70634565e-05 -6.44266381e-04]\n",
      "Weights on iteration  380 :  [ 6.03809594  0.47791212 -0.55829106  0.61977467  0.3628793   0.79237385\n",
      " -3.24803701 -0.48470782]\n",
      "Loss:  0.5981934211491128\n",
      "Weight derivatives:  [ 1.25440824e-04 -1.47600859e-04 -8.90307644e-05 -4.46850251e-04\n",
      " -1.46956531e-05  1.24297742e-03 -5.44459299e-05 -6.15794786e-04]\n",
      "Weights on iteration  390 :  [ 6.03734136  0.47880706 -0.55775468  0.62251373  0.36296722  0.78479214\n",
      " -3.24773001 -0.48093683]\n",
      "Loss:  0.5981792608153945\n",
      "Weight derivatives:  [ 1.24909281e-04 -1.44525676e-04 -8.82262767e-05 -4.28313230e-04\n",
      " -1.47503009e-05  1.20282565e-03 -6.06547820e-05 -5.91264665e-04]\n",
      "Weights on iteration  400 :  [ 6.03666563  0.47959486 -0.55727629  0.62487071  0.36304676  0.7782011\n",
      " -3.2474172  -0.47768558]\n",
      "Loss:  0.5981673992281286\n",
      "Bias:  1.2779677401242653\n"
     ]
    }
   ],
   "source": [
    "weights_old = np.array([ 0.7922078, 0.45199038, -0.08072242, 0.48454824, 0.505596, 0.5769389, 0.76302185, 0.50641925])\n",
    "weights_old_a = np.array([3.40068449, 0.4333755, -0.55011185, 0.22646619, 0.40094553, 1.27276349, -0.72093493, -0.46318842])\n",
    "weights = np.array([ 6.10375658, 0.43798321, -0.57367969, 0.22779496, 0.41117947, 1.41204838, -3.16402027, -0.85506265])\n",
    "bias = 1.4121690269083802\n",
    "\n",
    "def train_network(iterations, weights, bias):\n",
    "\n",
    "    # Alpha is step size (learning rate)\n",
    "    alpha = 0.6\n",
    "\n",
    "    # m is the amount of training data\n",
    "    m = training_data.shape[1]\n",
    "\n",
    "    for j in range(iterations):\n",
    "        \n",
    "        # l is the average of all the losses\n",
    "        l = 0.0\n",
    "        \n",
    "        # all derivitive weights and deriviative bias start at 0\n",
    "        derivative_weights = np.zeros(training_data.shape[0])\n",
    "        db = 0.0\n",
    "\n",
    "        for i in range(m):\n",
    "            # grab features\n",
    "            features = training_data[0:8, i]\n",
    "            # make prediction\n",
    "            yhat = sigmoid(features, weights, bias)\n",
    "            # get actual value\n",
    "            y = raw_training_data[8, i]\n",
    "            # calculate the loss\n",
    "            l += loss(y, yhat)\n",
    "            # find the derivative of the activation function\n",
    "            dz = dz_calc(yhat, y)\n",
    "            # calculate the deriviative of each weight and the bias\n",
    "            for k in range(len(derivative_weights)):\n",
    "                derivative_weights[k] += features[k] * dz\n",
    "            db += dz\n",
    "\n",
    "        # Calculate average for loss, derivarive bias, and all derivative weights\n",
    "        l /= m\n",
    "        for i in range(len(derivative_weights)):\n",
    "            derivative_weights[i] /= m\n",
    "        db /= m\n",
    "        \n",
    "        store_loss(l, j)\n",
    "\n",
    "        # Update weights with the derivative weight by learning rate\n",
    "        for i in range(len(weights)):\n",
    "            weights[i] = weights[i]-alpha*derivative_weights[i]\n",
    "        bias = bias - alpha*db\n",
    "        \n",
    "        \n",
    "\n",
    "        if j%10 == 0:\n",
    "            print(\"Weight derivatives: \", derivative_weights)\n",
    "            print(\"Weights on iteration \", j, \": \", weights)\n",
    "            print(\"Loss: \", l)\n",
    "        \n",
    "        elif j == iterations-1:\n",
    "            print(\"Weight derivatives: \", derivative_weights)\n",
    "            print(\"Weights on iteration \", j+1, \": \", weights)\n",
    "            print(\"Loss: \", l)\n",
    "            print(\"Bias: \", bias)\n",
    "        \n",
    "\n",
    "# clear loss_vec && iterations_vec\n",
    "iteration_vec = []\n",
    "loss_vec = []\n",
    "\n",
    "train_network(400, weights, bias)\n",
    "last_train = weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good predictions:  185\n",
      "Total predictions:  268\n",
      "69.02985074626866 %\n"
     ]
    }
   ],
   "source": [
    "def test_weights(weights, bias):\n",
    "    m = testing_data.shape[1]\n",
    "    good_pred = 0\n",
    "\n",
    "    for i in range(m):\n",
    "        features = testing_data[0:8, i]\n",
    "        # make prediction\n",
    "        yhat = sigmoid(features, weights, bias)\n",
    "        #print(\"Prediction\", yhat)\n",
    "        # get actual value\n",
    "        y = raw_testing_data[8, i]\n",
    "        #print(\"Actual\", y)\n",
    "\n",
    "        #if np.isclose(y, yhat):\n",
    "        #    print(\"TRUE\")\n",
    "        #    good_pred += 1\n",
    "        \n",
    "        if yhat < 0.5 and y == 0.0:\n",
    "            good_pred += 1\n",
    "        elif yhat >= 0.5 and y == 1.0:\n",
    "            good_pred += 1\n",
    "\n",
    "    print(\"Good predictions: \", good_pred)\n",
    "    print(\"Total predictions: \", m)\n",
    "    \n",
    "    print(good_pred/m*100, \"%\")\n",
    "        \n",
    "test_weights(weights, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x24984e76040>]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as pyplot\n",
    "\n",
    "pyplot.plot(iteration_vec, loss_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
